{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use only GPU 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from torch.optim import Adam\n",
    "from accelerate import Accelerator\n",
    "import wandb\n",
    "import gc\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.meteor_score import meteor_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data, validation data and testing data from IntentKonen dataset\n",
    "\n",
    "training_data = pd.read_csv('../../Dataset/train.csv')\n",
    "testing_data = pd.read_csv('../../Dataset/test.csv')\n",
    "validation_data = pd.read_csv('../../Dataset/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialoGPTDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "        #Loading the small version of DialoGPT tokenizer so that it is easier to run\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\", token='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')\n",
    "\n",
    "        #Input attributes from the dataset\n",
    "        self.input_attributes = ['hatespeech', 'csType']\n",
    "\n",
    "        #Output attributes from the dataset\n",
    "        self.output_attributes = ['counterspeech']\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        #Format used for framing the input text for tokenization\n",
    "        input_text = f'Hate: {row[\"hatespeech\"]} Type: {row[\"csType\"]}'\n",
    "        counter_speech = row[\"counterspeech\"]\n",
    "\n",
    "        input_ids = self.tokenizer.encode(input_text, return_tensors='pt', max_length=128, truncation=True, padding=\"max_length\")\n",
    "        counter_speech_ids = self.tokenizer.encode(counter_speech, return_tensors='pt', max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
    "            \n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9532\n",
      "2971\n",
      "1470\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DialoGPTDataset(training_data)\n",
    "test_dataset = DialoGPTDataset(testing_data)\n",
    "validation_dataset = DialoGPTDataset(validation_data)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"trainer_final_checkpoint\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"trainer_final_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell contains the actual training arguments and the Trainer object that was used to train the model and this was the same arguments used in the paper\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     num_train_epochs=20,\n",
    "#     per_device_train_batch_size=32,\n",
    "#     learning_rate=8e-5,\n",
    "#     weight_decay=0.03,\n",
    "#     save_strategy=\"epoch\",\n",
    "#     logging_dir='./logs',\n",
    "#     logging_steps=10,\n",
    "#     output_dir='./dialogpt_logs',\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=dialogpt_model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=validation_dataset,\n",
    "#     optimizers=(optimizer, None)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1492724/566792305.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "#Place holder training argument just to load the model\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./trainer_checkpoints',\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    num_train_epochs=3,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:   0%|                                                                     | 0/2971 [00:00<?, ?it/s]/tmp/ipykernel_1492724/2066604757.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_1492724/2066604757.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Processing Sentences: 100%|██████████████████████████████████████████████████████████| 2971/2971 [01:40<00:00, 29.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference completed for 2971 sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm  # For progress bar\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Create DataLoader with smaller batch size\n",
    "batch_size = 8\n",
    "dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "all_predictions = []\n",
    "all_references = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Get the total number of sentences\n",
    "total_sentences = len(test_dataset)\n",
    "\n",
    "# Batch inference with memory clearing and progress tracking\n",
    "with torch.no_grad():\n",
    "    completed_sentences = 0\n",
    "    with tqdm(total=total_sentences, desc=\"Processing Sentences\") as pbar:\n",
    "        for batch in dataloader:\n",
    "            # Move batch to GPU\n",
    "            batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "\n",
    "            # Perform inference\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            # Move tensors back to CPU immediately to free GPU memory\n",
    "            all_predictions.append(outputs.logits.cpu())\n",
    "            all_references.append(batch['labels'].cpu())\n",
    "\n",
    "            # Update progress bar\n",
    "            batch_size = batch['input_ids'].size(0)\n",
    "            completed_sentences += batch_size\n",
    "            pbar.update(batch_size)\n",
    "\n",
    "            # Force memory release\n",
    "            del batch\n",
    "            del outputs\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "# Print completion message\n",
    "print(f\"Inference completed for {completed_sentences} sentences.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate predictions and references\n",
    "predictions = torch.cat(all_predictions)\n",
    "references = torch.cat(all_references)\n",
    "\n",
    "predictions = predictions.squeeze(1)  # Shape: (total_samples, 128)\n",
    "references = references.squeeze(1)\n",
    "\n",
    "predictions = predictions.argmax(dim=-1)\n",
    "\n",
    "# Decode predictions and references\n",
    "decoded_predictions = test_dataset.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "decoded_references = test_dataset.tokenizer.batch_decode(references, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 0.1297\n",
      "ROUGE-2: 0.0030\n",
      "ROUGE-L: 0.1045\n",
      "METEOR: 0.0404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4ee2134d2245ccb789550e5dbc48b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc22207ec73c4df19ccb64c4a77001f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 9.19 seconds, 323.26 sentences/sec\n",
      "BERTScore - P: 0.7914, R: 0.8076, F1: 0.7993\n"
     ]
    }
   ],
   "source": [
    "rouge_scores = [scorer.score(ref, pred) for ref, pred in zip(decoded_references, decoded_predictions)]\n",
    "rouge1 = np.mean([score['rouge1'].fmeasure for score in rouge_scores])\n",
    "rouge2 = np.mean([score['rouge2'].fmeasure for score in rouge_scores])\n",
    "rougeL = np.mean([score['rougeL'].fmeasure for score in rouge_scores])\n",
    "\n",
    "print(f\"ROUGE-1: {rouge1:.4f}\")\n",
    "print(f\"ROUGE-2: {rouge2:.4f}\")\n",
    "print(f\"ROUGE-L: {rougeL:.4f}\")\n",
    "\n",
    "# --------------------------------\n",
    "# Calculate METEOR score\n",
    "# --------------------------------\n",
    "meteor_scores = []\n",
    "for ref, pred in zip(decoded_references, decoded_predictions):\n",
    "    # Tokenize both reference and hypothesis (prediction)\n",
    "    tokenized_ref = ref.split()\n",
    "    tokenized_pred = pred.split()\n",
    "    meteor_scores.append(meteor_score([tokenized_ref], tokenized_pred))\n",
    "avg_meteor = np.mean(meteor_scores)\n",
    "print(f\"METEOR: {avg_meteor:.4f}\")\n",
    "# --------------------------------\n",
    "# Calculate BERTScore\n",
    "# --------------------------------\n",
    "P, R, F1 = bert_score(cands=decoded_predictions, refs=decoded_references, lang='en', verbose=True)\n",
    "print(f\"BERTScore - P: {P.mean():.4f}, R: {R.mean():.4f}, F1: {F1.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "judge_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "judge_model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\").to(device)\n",
    "judge_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Counterspeech: 100%|████████████████████████████████████████████████████████| 549/549 [02:59<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating with Judge LLM...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with Judge LLM:   0%|                                                                   | 0/1097 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Scoring with Judge LLM: 100%|████████████████████████████████████████████████████████| 1097/1097 [00:42<00:00, 26.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Category Accuracy using Judge LLM: 0.6809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fixed_intents = [\"informative\", \"questioning\", \"denouncing\", \"positive\"]\n",
    "\n",
    "testing_data = pd.read_csv('../../Dataset/test.csv')\n",
    "\n",
    "gold_label_map = {}\n",
    "for _, row in testing_data.iterrows():\n",
    "    hs = row['hatespeech']\n",
    "    intent = row['csType'].lower()\n",
    "    if hs not in gold_label_map:\n",
    "        gold_label_map[hs] = set()\n",
    "    gold_label_map[hs].add(intent)\n",
    "\n",
    "hs_intent_pairs = []\n",
    "all_inputs = []\n",
    "\n",
    "for hs in gold_label_map:\n",
    "    for intent in fixed_intents:\n",
    "        prompt = f\"Hate: {hs} Type: {intent}\"\n",
    "        hs_intent_pairs.append((hs, intent))\n",
    "        all_inputs.append(prompt)\n",
    "\n",
    "generated_cs = {}\n",
    "batch_size = 8\n",
    "dataloader = DataLoader(all_inputs, batch_size=batch_size)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "tokenizer.padding_side = 'left'\n",
    "generated_texts = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Generating Counterspeech\"):\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "        outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True, pad_token_id=tokenizer.eos_token_id)\n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        generated_texts.extend(decoded)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "# Organize generated responses by hate speech\n",
    "for (hs, intent), response in zip(hs_intent_pairs, generated_texts):\n",
    "    if hs not in generated_cs:\n",
    "        generated_cs[hs] = {}\n",
    "    generated_cs[hs][intent] = response\n",
    "\n",
    "# Evaluate using Judge LLM\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "print(\"\\nEvaluating with Judge LLM...\\n\")\n",
    "for hs, intent_responses in tqdm(generated_cs.items(), desc=\"Scoring with Judge LLM\"):\n",
    "    intent_scores = {}\n",
    "    gold_labels = gold_label_map.get(hs, [])\n",
    "\n",
    "    for intent, cs in intent_responses.items():\n",
    "        prompt = (\n",
    "            f\"Hate speech: {hs}\\n\"\n",
    "            f\"Proposed counterspeech (intent: {intent}): {cs}\\n\\n\"\n",
    "            f\"On a scale of 1 to 10, how appropriate and effective is this counterspeech in response to the hate speech? Just respond with a number.\"\n",
    "        )\n",
    "\n",
    "        input_ids = judge_tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "        output_ids = judge_model.generate(input_ids, max_new_tokens=10, pad_token_id=judge_tokenizer.eos_token_id)\n",
    "        score_text = judge_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        try:\n",
    "            score = next((float(s) for s in score_text.split() if s.replace('.', '', 1).isdigit()), 0)\n",
    "            score = max(0, min(score, 10))\n",
    "        except:\n",
    "            score = 0\n",
    "\n",
    "        intent_scores[intent] = score\n",
    "\n",
    "    # Select best intent by highest score\n",
    "    best_intent = max(intent_scores, key=intent_scores.get)\n",
    "\n",
    "    if best_intent in gold_labels:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "# Final Accuracy\n",
    "accuracy = correct / total if total else 0\n",
    "print(f\"\\nFinal Category Accuracy using Judge LLM: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics:\n",
    "# Final Category Accuracy using Judge LLM: 0.6809\n",
    "# ROUGE-1: 0.1297\n",
    "# ROUGE-2: 0.0030\n",
    "# ROUGE-L: 0.1045\n",
    "# METEOR: 0.0404\n",
    "# BERTScore - P: 0.7914, R: 0.8076, F1: 0.7993"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
