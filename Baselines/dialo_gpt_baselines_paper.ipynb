{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QJ-yxJ6P_6dU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from torch.optim import Adam\n",
    "from accelerate import Accelerator\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# meteor = load(\"meteor\")\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/dhruv/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnoel22338\u001b[0m (\u001b[33mnlp_project_team\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login --relogin --verify ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "B6MBEOJg_6dV"
   },
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('../Dataset/train.csv')\n",
    "testing_data = pd.read_csv('../Dataset/test.csv')\n",
    "validation_data = pd.read_csv('../Dataset/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 865
    },
    "id": "KPdXrxQg_6dW",
    "outputId": "cdf009a0-850c-4a00-8dda-cc2e1a069901"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>csType</th>\n",
       "      <th>counterspeech</th>\n",
       "      <th>Suggest</th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Aggressive</th>\n",
       "      <th>Complexity</th>\n",
       "      <th>Comments</th>\n",
       "      <th>source</th>\n",
       "      <th>claim</th>\n",
       "      <th>...</th>\n",
       "      <th>hatespeechTarget</th>\n",
       "      <th>powerDynamics</th>\n",
       "      <th>prompt_offensiveness</th>\n",
       "      <th>prompt_target_group</th>\n",
       "      <th>prompt_speaker_intent</th>\n",
       "      <th>prompt_power_dynamics</th>\n",
       "      <th>prompt_implication</th>\n",
       "      <th>prompt_emotional_reaction</th>\n",
       "      <th>prompt_cognitive_reaction</th>\n",
       "      <th>prompt_cs_generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maybe the UN could talk to those asian and afr...</td>\n",
       "      <td>Informative</td>\n",
       "      <td>The us is the second most polluting country in...</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Human</td>\n",
       "      <td>The UN should focus on Asian and African natio...</td>\n",
       "      <td>...</td>\n",
       "      <td>asian_people</td>\n",
       "      <td>racial</td>\n",
       "      <td>Analyze the offensiveness of the statement: Ma...</td>\n",
       "      <td>Identify the group of people that the speaker ...</td>\n",
       "      <td>Analyze the speaker's intention behind writing...</td>\n",
       "      <td>Explain the underlying power dynamics between ...</td>\n",
       "      <td>Explain the implied meaning underlying the off...</td>\n",
       "      <td>Describe how the target group might feel emoti...</td>\n",
       "      <td>Describe how the target group might react cogn...</td>\n",
       "      <td>Analyze the different aspects such as offensiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maybe the UN could talk to those asian and afr...</td>\n",
       "      <td>Questioning</td>\n",
       "      <td>Doesn't everyone on the planet have a responsi...</td>\n",
       "      <td></td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Human</td>\n",
       "      <td>The UN should focus on Asian and African natio...</td>\n",
       "      <td>...</td>\n",
       "      <td>asian_people</td>\n",
       "      <td>racial</td>\n",
       "      <td>Analyze the offensiveness of the statement: Ma...</td>\n",
       "      <td>Identify the group of people that the speaker ...</td>\n",
       "      <td>Analyze the speaker's intention behind writing...</td>\n",
       "      <td>Explain the underlying power dynamics between ...</td>\n",
       "      <td>Explain the implied meaning underlying the off...</td>\n",
       "      <td>Describe how the target group might feel emoti...</td>\n",
       "      <td>Describe how the target group might react cogn...</td>\n",
       "      <td>Analyze the different aspects such as offensiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Maybe the UN could talk to those asian and afr...</td>\n",
       "      <td>Denouncing</td>\n",
       "      <td>The world would be a better place if people we...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Human</td>\n",
       "      <td>The UN should focus on Asian and African natio...</td>\n",
       "      <td>...</td>\n",
       "      <td>asian_people</td>\n",
       "      <td>racial</td>\n",
       "      <td>Analyze the offensiveness of the statement: Ma...</td>\n",
       "      <td>Identify the group of people that the speaker ...</td>\n",
       "      <td>Analyze the speaker's intention behind writing...</td>\n",
       "      <td>Explain the underlying power dynamics between ...</td>\n",
       "      <td>Explain the implied meaning underlying the off...</td>\n",
       "      <td>Describe how the target group might feel emoti...</td>\n",
       "      <td>Describe how the target group might react cogn...</td>\n",
       "      <td>Analyze the different aspects such as offensiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maybe the UN could talk to those asian and afr...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>You're right, ocean pollution is one of the ma...</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Human</td>\n",
       "      <td>The UN should focus on Asian and African natio...</td>\n",
       "      <td>...</td>\n",
       "      <td>asian_people</td>\n",
       "      <td>racial</td>\n",
       "      <td>Analyze the offensiveness of the statement: Ma...</td>\n",
       "      <td>Identify the group of people that the speaker ...</td>\n",
       "      <td>Analyze the speaker's intention behind writing...</td>\n",
       "      <td>Explain the underlying power dynamics between ...</td>\n",
       "      <td>Explain the implied meaning underlying the off...</td>\n",
       "      <td>Describe how the target group might feel emoti...</td>\n",
       "      <td>Describe how the target group might react cogn...</td>\n",
       "      <td>Analyze the different aspects such as offensiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Juice Jews are worse that nukes. Caption to a ...</td>\n",
       "      <td>Informative</td>\n",
       "      <td>Anti-semitism is a serious problem that we nee...</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Human</td>\n",
       "      <td>Jews are worse than nukes.</td>\n",
       "      <td>...</td>\n",
       "      <td>jews</td>\n",
       "      <td>religion</td>\n",
       "      <td>Analyze the offensiveness of the statement: Ju...</td>\n",
       "      <td>Identify the group of people that the speaker ...</td>\n",
       "      <td>Analyze the speaker's intention behind writing...</td>\n",
       "      <td>Explain the underlying power dynamics between ...</td>\n",
       "      <td>Explain the implied meaning underlying the off...</td>\n",
       "      <td>Describe how the target group might feel emoti...</td>\n",
       "      <td>Describe how the target group might react cogn...</td>\n",
       "      <td>Analyze the different aspects such as offensiv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          hatespeech       csType  \\\n",
       "0  Maybe the UN could talk to those asian and afr...  Informative   \n",
       "1  Maybe the UN could talk to those asian and afr...  Questioning   \n",
       "2  Maybe the UN could talk to those asian and afr...   Denouncing   \n",
       "3  Maybe the UN could talk to those asian and afr...     Positive   \n",
       "4  Juice Jews are worse that nukes. Caption to a ...  Informative   \n",
       "\n",
       "                                       counterspeech Suggest  Relevance  \\\n",
       "0  The us is the second most polluting country in...       3        4.0   \n",
       "1  Doesn't everyone on the planet have a responsi...                3.0   \n",
       "2  The world would be a better place if people we...       1        1.0   \n",
       "3  You're right, ocean pollution is one of the ma...       3        4.0   \n",
       "4  Anti-semitism is a serious problem that we nee...       2        3.0   \n",
       "\n",
       "   Aggressive  Complexity  Comments source  \\\n",
       "0         2.0         3.0       NaN  Human   \n",
       "1         2.0         2.0       NaN  Human   \n",
       "2         2.0         2.0       NaN  Human   \n",
       "3         2.0         2.0       NaN  Human   \n",
       "4         2.0         2.0       NaN  Human   \n",
       "\n",
       "                                               claim  ... hatespeechTarget  \\\n",
       "0  The UN should focus on Asian and African natio...  ...     asian_people   \n",
       "1  The UN should focus on Asian and African natio...  ...     asian_people   \n",
       "2  The UN should focus on Asian and African natio...  ...     asian_people   \n",
       "3  The UN should focus on Asian and African natio...  ...     asian_people   \n",
       "4                         Jews are worse than nukes.  ...             jews   \n",
       "\n",
       "  powerDynamics                               prompt_offensiveness  \\\n",
       "0        racial  Analyze the offensiveness of the statement: Ma...   \n",
       "1        racial  Analyze the offensiveness of the statement: Ma...   \n",
       "2        racial  Analyze the offensiveness of the statement: Ma...   \n",
       "3        racial  Analyze the offensiveness of the statement: Ma...   \n",
       "4      religion  Analyze the offensiveness of the statement: Ju...   \n",
       "\n",
       "                                 prompt_target_group  \\\n",
       "0  Identify the group of people that the speaker ...   \n",
       "1  Identify the group of people that the speaker ...   \n",
       "2  Identify the group of people that the speaker ...   \n",
       "3  Identify the group of people that the speaker ...   \n",
       "4  Identify the group of people that the speaker ...   \n",
       "\n",
       "                               prompt_speaker_intent  \\\n",
       "0  Analyze the speaker's intention behind writing...   \n",
       "1  Analyze the speaker's intention behind writing...   \n",
       "2  Analyze the speaker's intention behind writing...   \n",
       "3  Analyze the speaker's intention behind writing...   \n",
       "4  Analyze the speaker's intention behind writing...   \n",
       "\n",
       "                               prompt_power_dynamics  \\\n",
       "0  Explain the underlying power dynamics between ...   \n",
       "1  Explain the underlying power dynamics between ...   \n",
       "2  Explain the underlying power dynamics between ...   \n",
       "3  Explain the underlying power dynamics between ...   \n",
       "4  Explain the underlying power dynamics between ...   \n",
       "\n",
       "                                  prompt_implication  \\\n",
       "0  Explain the implied meaning underlying the off...   \n",
       "1  Explain the implied meaning underlying the off...   \n",
       "2  Explain the implied meaning underlying the off...   \n",
       "3  Explain the implied meaning underlying the off...   \n",
       "4  Explain the implied meaning underlying the off...   \n",
       "\n",
       "                           prompt_emotional_reaction  \\\n",
       "0  Describe how the target group might feel emoti...   \n",
       "1  Describe how the target group might feel emoti...   \n",
       "2  Describe how the target group might feel emoti...   \n",
       "3  Describe how the target group might feel emoti...   \n",
       "4  Describe how the target group might feel emoti...   \n",
       "\n",
       "                           prompt_cognitive_reaction  \\\n",
       "0  Describe how the target group might react cogn...   \n",
       "1  Describe how the target group might react cogn...   \n",
       "2  Describe how the target group might react cogn...   \n",
       "3  Describe how the target group might react cogn...   \n",
       "4  Describe how the target group might react cogn...   \n",
       "\n",
       "                                prompt_cs_generation  \n",
       "0  Analyze the different aspects such as offensiv...  \n",
       "1  Analyze the different aspects such as offensiv...  \n",
       "2  Analyze the different aspects such as offensiv...  \n",
       "3  Analyze the different aspects such as offensiv...  \n",
       "4  Analyze the different aspects such as offensiv...  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofLk7bt7_6dW",
    "outputId": "c3baccf5-4884-43dc-bab1-3124b899ac34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['hatespeech', 'csType', 'counterspeech', 'Suggest', 'Relevance',\n",
      "       'Aggressive', 'Complexity', 'Comments', 'source', 'claim',\n",
      "       'centralTopic', 'speakerIntent', 'targetGroup', 'relevantPowerDynamics',\n",
      "       'hatespeechImplication', 'targetGroupEmotionalReaction',\n",
      "       'targetGroupCognitiveReaction', 'hatespeechOffensiveness', 'id',\n",
      "       'is_high_quality', 'hs_id', 'hatespeechTarget', 'powerDynamics',\n",
      "       'prompt_offensiveness', 'prompt_target_group', 'prompt_speaker_intent',\n",
      "       'prompt_power_dynamics', 'prompt_implication',\n",
      "       'prompt_emotional_reaction', 'prompt_cognitive_reaction',\n",
      "       'prompt_cs_generation'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "columns = training_data.columns\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "smSz_WAB_6dX"
   },
   "outputs": [],
   "source": [
    "class DialoGPTDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\", token='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')\n",
    "        self.input_attributes = ['hatespeech', 'csType']\n",
    "        self.output_attributes = ['counterspeech']\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        input_text = f'Hate: {row[\"hatespeech\"]} Type: {row[\"csType\"]}'\n",
    "        counter_speech = row[\"counterspeech\"]\n",
    "\n",
    "        input_ids = self.tokenizer.encode(input_text, return_tensors='pt', max_length=128, truncation=True, padding=\"max_length\")\n",
    "        counter_speech_ids = self.tokenizer.encode(counter_speech, return_tensors='pt', max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjDAavD0_6dX",
    "outputId": "273a2334-5c66-4d33-c435-514c9ab5d936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9532\n",
      "2971\n",
      "1470\n",
      "{'input_ids': tensor([[   39,   378,    25,  6674,   262,  4725,   714,  1561,   284,   883,\n",
      "           355,   666,   290,  6580, 37189,  7027,  4497,   329,  4101,    10,\n",
      "           286,   262, 12231,   287,   262, 23744,  2427,   286, 23630,   319,\n",
      "           428, 20041,   546,  4258,  1487,    13,  5994,    25, 45255,   876,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'labels': tensor([[  464,   514,   318,   262,  1218,   749,  3278, 15129,  1499,   287,\n",
      "           262,   995,   532,   355,   262,   995,   447,   247,    82,  4094,\n",
      "          7593,   290,  5068,  1176,    13,   356,   389,   477,   284,  8138,\n",
      "           994,   290,   761,   284,   670,  1021,   287,  1021,   284,  2251,\n",
      "         13347,  1487,    11,   355,  6886,   284, 10609,   262,  7660,   284,\n",
      "          1854,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_537618/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_537618/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DialoGPTDataset(training_data)\n",
    "test_dataset = DialoGPTDataset(testing_data)\n",
    "validation_dataset = DialoGPTDataset(validation_data)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(validation_dataset))\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t5ZpRufe_6dX",
    "outputId": "6cb1801a-64a6-479f-fdc8-83f5fa4ef13e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/miniforge3/envs/nlp_project/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdbb3bfb07b648be8241833e20317c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/641 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1982f723b374453b7e26cc517150483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/351M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f3a9d3c087455699acbf36258dd51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dialogpt_model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\", use_auth_token='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')\n",
    "optimizer = Adam(dialogpt_model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kHCturNK_6dY"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=32,\n",
    "    learning_rate=8e-5,\n",
    "    weight_decay=0.03,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    output_dir='./dialogpt_logs',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "laJosVqU_6dY"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=dialogpt_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    optimizers=(optimizer, None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "1FItl6qiBEu5",
    "outputId": "4f38f517-6ff9-4efc-d41c-74a91d055df3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnoel22338\u001b[0m (\u001b[33mnlp_project_team\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dhruv/Roamify/Noel SDOS/NLP-Project-CounterSpeech/Baselines/wandb/run-20250323_134242-ib3538a3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp_project_team/huggingface/runs/ib3538a3' target=\"_blank\">./dialogpt_logs</a></strong> to <a href='https://wandb.ai/nlp_project_team/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp_project_team/huggingface' target=\"_blank\">https://wandb.ai/nlp_project_team/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp_project_team/huggingface/runs/ib3538a3' target=\"_blank\">https://wandb.ai/nlp_project_team/huggingface/runs/ib3538a3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5960' max='5960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5960/5960 36:13, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.352600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.125800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.416800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.215900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.030300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.883500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.896600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.679000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.753100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.579100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.518200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.549800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.551800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.699400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.353300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.505400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.315100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.482900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.439800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.544300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.460700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.344500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.379100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.448900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.428100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.415900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.422100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.284600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.309300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.316700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.422700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.275600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.379400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.340500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.312100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.395900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.205700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.208900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.261000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.495900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.359100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.322700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.381100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.379800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.325900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>2.410800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.336300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.371900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.240300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.266300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.313900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>2.266400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.261900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>2.378200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.257100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>2.219500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.313500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>2.382300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.317200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.415500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.247600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>2.083400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.221400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>2.174800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.255800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>2.308400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>2.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.273100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.210900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>2.202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.283600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>2.232500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.317600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>2.158800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>2.266700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>2.228500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>2.381700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.194100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.231500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>2.258000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>2.268800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>2.352600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>2.229900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>2.316400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>2.290900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>2.087700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.281500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>2.039400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>2.113600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>2.075300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>2.316300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.289700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>2.129200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>2.261100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>2.177600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>2.165500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.156200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>2.261800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>2.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>2.164900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>2.401200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.255200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>2.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>2.291800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>2.430200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>2.327400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.214600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>2.271400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>2.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>2.322000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>2.241500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.246600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>2.353100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>2.041800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>2.245100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>2.166200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.231100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>2.170300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>2.206200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>2.040400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>2.300200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.192300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>2.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>2.215300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>2.231600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>2.157400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.286900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>2.242800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>2.262700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>1.984200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>2.282700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.218500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>2.153800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>2.142100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>2.090700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>2.340500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>2.223100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>2.299100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>2.298200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>2.332500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>2.205200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.207400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>2.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>2.150300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>2.170500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>2.323500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>2.252500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>2.307500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>2.240400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>2.123800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>2.234900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.168600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>2.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>2.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>2.168800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>2.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>2.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>2.206900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>2.293700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>2.188300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>2.170600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>2.208100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>2.041100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>2.263400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>2.090900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>2.178100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>2.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>2.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>2.235700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>2.174400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>2.264400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>2.172800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>2.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>2.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>2.084900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>2.245500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>2.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>2.241600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>2.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>2.116600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>2.302600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>2.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>2.235400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>2.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>2.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>2.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>2.251100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.131700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>2.204100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>2.173700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>2.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>2.297200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>2.146800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>2.036900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>2.122300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>2.123800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>2.286900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>2.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>2.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>2.191500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>2.129700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>2.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>2.137400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2170</td>\n",
       "      <td>2.143200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>2.307200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2190</td>\n",
       "      <td>2.119400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.099300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>2.298700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>2.259400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2230</td>\n",
       "      <td>2.121400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>2.174800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>2.074600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>2.258300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2270</td>\n",
       "      <td>2.212200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>2.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2290</td>\n",
       "      <td>2.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.157400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>2.248200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>2.114300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>2.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>2.187700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>2.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>2.103300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2370</td>\n",
       "      <td>2.440400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>2.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2390</td>\n",
       "      <td>2.174500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2410</td>\n",
       "      <td>2.267800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>2.143100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2430</td>\n",
       "      <td>2.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>2.068600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>2.278500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>2.153800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2470</td>\n",
       "      <td>2.152600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>2.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2490</td>\n",
       "      <td>2.153300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.059100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2510</td>\n",
       "      <td>2.190900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>2.208800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2530</td>\n",
       "      <td>2.151500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>2.078200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>2.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>2.057400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2570</td>\n",
       "      <td>2.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>2.134300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2590</td>\n",
       "      <td>2.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.229600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2610</td>\n",
       "      <td>2.284300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>2.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2630</td>\n",
       "      <td>2.187200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>2.220900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>2.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>2.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2670</td>\n",
       "      <td>2.154400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>2.174100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2690</td>\n",
       "      <td>2.253400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.154100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2710</td>\n",
       "      <td>2.185700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>2.110800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2730</td>\n",
       "      <td>2.137200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>2.201800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>2.124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>2.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2770</td>\n",
       "      <td>2.289500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>2.259400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2790</td>\n",
       "      <td>2.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.962600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2810</td>\n",
       "      <td>2.221500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>2.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2830</td>\n",
       "      <td>2.108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>2.193500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>2.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>2.156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2870</td>\n",
       "      <td>2.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>2.201800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2890</td>\n",
       "      <td>2.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2910</td>\n",
       "      <td>2.234900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>2.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2930</td>\n",
       "      <td>2.168800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>2.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>2.164900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>2.165500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2970</td>\n",
       "      <td>2.216200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>2.175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2990</td>\n",
       "      <td>2.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.222600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3010</td>\n",
       "      <td>2.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>2.229400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3030</td>\n",
       "      <td>2.089400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>2.178200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>2.104200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>2.137300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3070</td>\n",
       "      <td>2.189200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>2.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3090</td>\n",
       "      <td>2.162100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.191900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3110</td>\n",
       "      <td>2.283500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>2.113600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3130</td>\n",
       "      <td>2.132900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>2.171900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>2.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>2.133700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3170</td>\n",
       "      <td>2.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3180</td>\n",
       "      <td>2.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3190</td>\n",
       "      <td>2.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.188700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3210</td>\n",
       "      <td>2.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>2.091100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3230</td>\n",
       "      <td>2.195500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>2.110800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>2.210900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>2.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3270</td>\n",
       "      <td>2.139500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>2.124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3290</td>\n",
       "      <td>2.100200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.213800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3310</td>\n",
       "      <td>2.212500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>2.253400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3330</td>\n",
       "      <td>2.144400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3340</td>\n",
       "      <td>2.165100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>2.044900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>2.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3370</td>\n",
       "      <td>2.072200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3380</td>\n",
       "      <td>2.178500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3390</td>\n",
       "      <td>2.340700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3410</td>\n",
       "      <td>2.154900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3420</td>\n",
       "      <td>2.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3430</td>\n",
       "      <td>2.252500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>2.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>2.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3460</td>\n",
       "      <td>2.120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3470</td>\n",
       "      <td>2.074600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>2.066800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3490</td>\n",
       "      <td>2.269000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.174100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3510</td>\n",
       "      <td>2.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>2.181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3530</td>\n",
       "      <td>2.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3540</td>\n",
       "      <td>2.116500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>2.162700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3560</td>\n",
       "      <td>2.162200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3570</td>\n",
       "      <td>2.140300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3580</td>\n",
       "      <td>2.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3590</td>\n",
       "      <td>2.247300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.148700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3610</td>\n",
       "      <td>2.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3620</td>\n",
       "      <td>2.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3630</td>\n",
       "      <td>2.177900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>2.156300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>2.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3660</td>\n",
       "      <td>2.240400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3670</td>\n",
       "      <td>1.972800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>2.233600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3690</td>\n",
       "      <td>2.128400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.126300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3710</td>\n",
       "      <td>2.168500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3720</td>\n",
       "      <td>2.224400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3730</td>\n",
       "      <td>2.157700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3740</td>\n",
       "      <td>2.173300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>2.236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>2.074100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3770</td>\n",
       "      <td>2.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3780</td>\n",
       "      <td>2.175800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3790</td>\n",
       "      <td>2.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.108800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3810</td>\n",
       "      <td>2.084100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3820</td>\n",
       "      <td>2.137900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3830</td>\n",
       "      <td>2.071700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>2.184900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>2.084700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3860</td>\n",
       "      <td>2.234700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3870</td>\n",
       "      <td>2.210400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3880</td>\n",
       "      <td>2.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3890</td>\n",
       "      <td>2.145200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>2.096800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3910</td>\n",
       "      <td>2.218300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3920</td>\n",
       "      <td>2.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3930</td>\n",
       "      <td>2.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3940</td>\n",
       "      <td>2.249800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>2.066800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3960</td>\n",
       "      <td>2.041400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3970</td>\n",
       "      <td>2.130100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3980</td>\n",
       "      <td>2.160600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3990</td>\n",
       "      <td>2.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.237800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4010</td>\n",
       "      <td>1.980700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4020</td>\n",
       "      <td>2.140300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4030</td>\n",
       "      <td>2.202200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4040</td>\n",
       "      <td>2.098700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>2.076800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4060</td>\n",
       "      <td>2.169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4070</td>\n",
       "      <td>2.145400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4080</td>\n",
       "      <td>2.254600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4090</td>\n",
       "      <td>2.169800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>2.199800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4110</td>\n",
       "      <td>2.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4120</td>\n",
       "      <td>2.117800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4130</td>\n",
       "      <td>2.214700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4140</td>\n",
       "      <td>2.147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>2.141600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4160</td>\n",
       "      <td>2.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4170</td>\n",
       "      <td>2.145200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4180</td>\n",
       "      <td>2.036800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4190</td>\n",
       "      <td>2.219600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.235300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4210</td>\n",
       "      <td>2.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4220</td>\n",
       "      <td>2.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4230</td>\n",
       "      <td>2.075600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4240</td>\n",
       "      <td>2.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>2.149600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4260</td>\n",
       "      <td>2.284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4270</td>\n",
       "      <td>1.977100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4280</td>\n",
       "      <td>2.160900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4290</td>\n",
       "      <td>2.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>2.109800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4310</td>\n",
       "      <td>2.078900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4320</td>\n",
       "      <td>2.284900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4330</td>\n",
       "      <td>2.145800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4340</td>\n",
       "      <td>2.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>2.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4360</td>\n",
       "      <td>2.214100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4370</td>\n",
       "      <td>2.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4380</td>\n",
       "      <td>2.118400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4390</td>\n",
       "      <td>1.978200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.258100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4410</td>\n",
       "      <td>2.296500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4420</td>\n",
       "      <td>2.251400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4430</td>\n",
       "      <td>2.074100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4440</td>\n",
       "      <td>2.156900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>2.127900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4460</td>\n",
       "      <td>2.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4470</td>\n",
       "      <td>2.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4480</td>\n",
       "      <td>2.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4490</td>\n",
       "      <td>2.147400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.082900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4510</td>\n",
       "      <td>2.202700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4520</td>\n",
       "      <td>2.123400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4530</td>\n",
       "      <td>2.102500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4540</td>\n",
       "      <td>2.057400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>2.204200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4560</td>\n",
       "      <td>2.263500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4570</td>\n",
       "      <td>2.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4580</td>\n",
       "      <td>2.129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4590</td>\n",
       "      <td>2.139600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.035800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4610</td>\n",
       "      <td>2.084600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4620</td>\n",
       "      <td>2.131200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4630</td>\n",
       "      <td>2.245500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4640</td>\n",
       "      <td>2.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>2.253900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4660</td>\n",
       "      <td>2.151800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4670</td>\n",
       "      <td>2.217800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4680</td>\n",
       "      <td>2.140200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4690</td>\n",
       "      <td>2.048900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>2.207300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4710</td>\n",
       "      <td>2.231300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4720</td>\n",
       "      <td>2.091500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4730</td>\n",
       "      <td>2.124300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4740</td>\n",
       "      <td>2.208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>2.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4760</td>\n",
       "      <td>2.291700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4770</td>\n",
       "      <td>2.058400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4780</td>\n",
       "      <td>1.987700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4790</td>\n",
       "      <td>2.233300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4810</td>\n",
       "      <td>2.122400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4820</td>\n",
       "      <td>2.283900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4830</td>\n",
       "      <td>2.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4840</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>2.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4860</td>\n",
       "      <td>2.148600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4870</td>\n",
       "      <td>2.126300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4880</td>\n",
       "      <td>2.144400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4890</td>\n",
       "      <td>2.142900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>2.231100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4910</td>\n",
       "      <td>2.046300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4920</td>\n",
       "      <td>2.177500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4930</td>\n",
       "      <td>2.170200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4940</td>\n",
       "      <td>2.227600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>2.172200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4960</td>\n",
       "      <td>2.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4970</td>\n",
       "      <td>2.147700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4980</td>\n",
       "      <td>2.136500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4990</td>\n",
       "      <td>2.196300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.150400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5010</td>\n",
       "      <td>2.164700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5020</td>\n",
       "      <td>2.081500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5030</td>\n",
       "      <td>2.166500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5040</td>\n",
       "      <td>2.174400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>2.089900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5060</td>\n",
       "      <td>2.100600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5070</td>\n",
       "      <td>2.183400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5080</td>\n",
       "      <td>2.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5090</td>\n",
       "      <td>2.217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>2.289900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5110</td>\n",
       "      <td>2.067700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5120</td>\n",
       "      <td>2.114900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5130</td>\n",
       "      <td>2.193300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5140</td>\n",
       "      <td>2.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>2.154500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5160</td>\n",
       "      <td>2.168600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5170</td>\n",
       "      <td>2.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5180</td>\n",
       "      <td>2.150600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5190</td>\n",
       "      <td>2.096400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.055900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5210</td>\n",
       "      <td>2.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5220</td>\n",
       "      <td>2.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5230</td>\n",
       "      <td>2.042100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5240</td>\n",
       "      <td>2.090200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>2.098200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5260</td>\n",
       "      <td>2.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5270</td>\n",
       "      <td>2.151400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5280</td>\n",
       "      <td>2.278200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5290</td>\n",
       "      <td>2.124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>2.077700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5310</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5320</td>\n",
       "      <td>2.191900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5330</td>\n",
       "      <td>2.127700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5340</td>\n",
       "      <td>2.225900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>2.071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5360</td>\n",
       "      <td>2.201300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5370</td>\n",
       "      <td>2.143800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5380</td>\n",
       "      <td>2.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5390</td>\n",
       "      <td>2.158800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5410</td>\n",
       "      <td>2.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5420</td>\n",
       "      <td>2.071800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5430</td>\n",
       "      <td>2.179100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5440</td>\n",
       "      <td>2.167600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>2.191500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5460</td>\n",
       "      <td>2.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5470</td>\n",
       "      <td>2.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5480</td>\n",
       "      <td>2.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5490</td>\n",
       "      <td>2.238100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.258200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5510</td>\n",
       "      <td>2.235100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5520</td>\n",
       "      <td>2.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5530</td>\n",
       "      <td>2.059700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5540</td>\n",
       "      <td>2.152800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>2.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5560</td>\n",
       "      <td>2.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5570</td>\n",
       "      <td>2.137800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5580</td>\n",
       "      <td>2.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5590</td>\n",
       "      <td>2.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.138600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5610</td>\n",
       "      <td>2.066800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5620</td>\n",
       "      <td>2.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5630</td>\n",
       "      <td>2.126500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5640</td>\n",
       "      <td>2.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>2.156200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5660</td>\n",
       "      <td>2.201800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5670</td>\n",
       "      <td>2.037900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5680</td>\n",
       "      <td>2.240400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5690</td>\n",
       "      <td>2.158800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>2.117400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5710</td>\n",
       "      <td>2.097600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5720</td>\n",
       "      <td>2.193500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5730</td>\n",
       "      <td>1.987400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5740</td>\n",
       "      <td>2.217100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>2.288500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5760</td>\n",
       "      <td>2.252200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5770</td>\n",
       "      <td>2.108600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5780</td>\n",
       "      <td>2.152200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5790</td>\n",
       "      <td>2.164200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.127600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5810</td>\n",
       "      <td>2.158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5820</td>\n",
       "      <td>2.128600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5830</td>\n",
       "      <td>2.106600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5840</td>\n",
       "      <td>2.067800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>2.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5860</td>\n",
       "      <td>2.066000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5870</td>\n",
       "      <td>2.148100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5880</td>\n",
       "      <td>2.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5890</td>\n",
       "      <td>2.167000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>2.056900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5910</td>\n",
       "      <td>2.090700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5920</td>\n",
       "      <td>2.086000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5930</td>\n",
       "      <td>2.178400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5940</td>\n",
       "      <td>2.057300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>2.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5960</td>\n",
       "      <td>2.111200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5960, training_loss=2.2047850445612966, metrics={'train_runtime': 2175.6188, 'train_samples_per_second': 87.626, 'train_steps_per_second': 2.739, 'total_flos': 1.245317824512e+16, 'train_loss': 2.2047850445612966, 'epoch': 20.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "qf2qFXqADJEA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_482576/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_482576/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='445' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [372/372 01:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0141243934631348, 'eval_runtime': 12.6969, 'eval_samples_per_second': 233.995, 'eval_steps_per_second': 29.299, 'epoch': 20.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate(test_dataset)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
