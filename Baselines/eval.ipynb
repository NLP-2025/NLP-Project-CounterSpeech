{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use only GPU 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from torch.optim import Adam\n",
    "from accelerate import Accelerator\n",
    "import wandb\n",
    "import gc\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# meteor = load('meteor')\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('../Dataset/train.csv')\n",
    "testing_data = pd.read_csv('../Dataset/test.csv')\n",
    "validation_data = pd.read_csv('../Dataset/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialoGPTDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\", token='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')\n",
    "        self.input_attributes = ['hatespeech', 'csType']\n",
    "        self.output_attributes = ['counterspeech']\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        input_text = f'Hate: {row[\"hatespeech\"]} Type: {row[\"csType\"]}'\n",
    "        counter_speech = row[\"counterspeech\"]\n",
    "\n",
    "        input_ids = self.tokenizer.encode(input_text, return_tensors='pt', max_length=128, truncation=True, padding=\"max_length\")\n",
    "        counter_speech_ids = self.tokenizer.encode(counter_speech, return_tensors='pt', max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9532\n",
      "2971\n",
      "1470\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DialoGPTDataset(training_data)\n",
    "test_dataset = DialoGPTDataset(testing_data)\n",
    "validation_dataset = DialoGPTDataset(validation_data)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"trainer_final_checkpoint\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"trainer_final_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_609704/3738237524.py:11: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./trainer_checkpoints',\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    num_train_epochs=3,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "# Create a new Trainer (without training)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:   0%|                                                                            | 0/2971 [00:00<?, ?it/s]/tmp/ipykernel_609704/4188864782.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_609704/4188864782.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Processing Sentences: 100%|█████████████████████████████████████████████████████████████████| 2971/2971 [01:44<00:00, 28.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference completed for 2971 sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm  # For progress bar\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Create DataLoader with smaller batch size\n",
    "batch_size = 8\n",
    "dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "all_predictions = []\n",
    "all_references = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Get the total number of sentences\n",
    "total_sentences = len(test_dataset)\n",
    "\n",
    "# Batch inference with memory clearing and progress tracking\n",
    "with torch.no_grad():\n",
    "    completed_sentences = 0\n",
    "    with tqdm(total=total_sentences, desc=\"Processing Sentences\") as pbar:\n",
    "        for batch in dataloader:\n",
    "            # Move batch to GPU\n",
    "            batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "\n",
    "            # Perform inference\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            # Move tensors back to CPU immediately to free GPU memory\n",
    "            all_predictions.append(outputs.logits.cpu())\n",
    "            all_references.append(batch['labels'].cpu())\n",
    "\n",
    "            # Update progress bar\n",
    "            batch_size = batch['input_ids'].size(0)\n",
    "            completed_sentences += batch_size\n",
    "            pbar.update(batch_size)\n",
    "\n",
    "            # Force memory release\n",
    "            del batch\n",
    "            del outputs\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "# Print completion message\n",
    "print(f\"Inference completed for {completed_sentences} sentences.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate predictions and references\n",
    "predictions = torch.cat(all_predictions)\n",
    "references = torch.cat(all_references)\n",
    "\n",
    "predictions = predictions.squeeze(1)  # Shape: (total_samples, 128)\n",
    "references = references.squeeze(1)\n",
    "\n",
    "predictions = predictions.argmax(dim=-1)\n",
    "\n",
    "# Decode predictions and references\n",
    "decoded_predictions = test_dataset.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "decoded_references = test_dataset.tokenizer.batch_decode(references, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 0.1297\n",
      "ROUGE-2: 0.0030\n",
      "ROUGE-L: 0.1045\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c527664b16d2432b84da340974c4cb38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4456474b33174fb5bf782e58c73e2faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72adfc19b17a47e7aab60fb7fe731f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f9cff20a7d4ad38f800dff36f44027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eef4fd2afcf41269ec8f95cb399e9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9208d98bb6274458a30203685123a3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb91dde9b6db49c4b508644e28bb436f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528edc0764234152b1d8c543479751cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 9.72 seconds, 305.56 sentences/sec\n",
      "BERTScore - P: 0.7914, R: 0.8076, F1: 0.7993\n"
     ]
    }
   ],
   "source": [
    "# Calculate ROUGE scores\n",
    "rouge_scores = [scorer.score(ref, pred) for ref, pred in zip(decoded_references, decoded_predictions)]\n",
    "rouge1 = np.mean([score['rouge1'].fmeasure for score in rouge_scores])\n",
    "print(f\"ROUGE-1: {rouge1:.4f}\")\n",
    "rouge2 = np.mean([score['rouge2'].fmeasure for score in rouge_scores])\n",
    "print(f\"ROUGE-2: {rouge2:.4f}\")\n",
    "rougeL = np.mean([score['rougeL'].fmeasure for score in rouge_scores])\n",
    "print(f\"ROUGE-L: {rougeL:.4f}\")\n",
    "\n",
    "# Calculate METEOR score\n",
    "# meteor_score = meteor.compute(predictions=decoded_predictions, references=decoded_references)['meteor']\n",
    "# print(f\"METEOR: {meteor_score:.4f}\")\n",
    "\n",
    "# Calculate BERTScore\n",
    "\n",
    "P, R, F1 = bert_score(cands=decoded_predictions, refs=decoded_references, lang='en', verbose=True)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"BERTScore - P: {P.mean():.4f}, R: {R.mean():.4f}, F1: {F1.mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
