{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use only GPU 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from torch.optim import Adam\n",
    "from accelerate import Accelerator\n",
    "import wandb\n",
    "import gc\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# meteor = load('meteor')\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data, validation data and testing data from IntentKonen dataset\n",
    "\n",
    "training_data = pd.read_csv('../Dataset/train.csv')\n",
    "testing_data = pd.read_csv('../Dataset/test.csv')\n",
    "validation_data = pd.read_csv('../Dataset/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialoGPTDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "        #Loading the small version of DialoGPT tokenizer so that it is easier to run\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\", token='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')\n",
    "\n",
    "        #Input attributes from the dataset\n",
    "        self.input_attributes = ['hatespeech', 'csType']\n",
    "\n",
    "        #Output attributes from the dataset\n",
    "        self.output_attributes = ['counterspeech']\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        #Format used for framing the input text for tokenization\n",
    "        input_text = f'Hate: {row[\"hatespeech\"]} Type: {row[\"csType\"]}'\n",
    "        counter_speech = row[\"counterspeech\"]\n",
    "\n",
    "        input_ids = self.tokenizer.encode(input_text, return_tensors='pt', max_length=128, truncation=True, padding=\"max_length\")\n",
    "        counter_speech_ids = self.tokenizer.encode(counter_speech, return_tensors='pt', max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9532\n",
      "2971\n",
      "1470\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DialoGPTDataset(training_data)\n",
    "test_dataset = DialoGPTDataset(testing_data)\n",
    "validation_dataset = DialoGPTDataset(validation_data)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"trainer_final_checkpoint\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"trainer_final_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell contains the actual training arguments and the Trainer object that was used to train the model and this was the same arguments used in the paper\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     num_train_epochs=20,\n",
    "#     per_device_train_batch_size=32,\n",
    "#     learning_rate=8e-5,\n",
    "#     weight_decay=0.03,\n",
    "#     save_strategy=\"epoch\",\n",
    "#     logging_dir='./logs',\n",
    "#     logging_steps=10,\n",
    "#     output_dir='./dialogpt_logs',\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=dialogpt_model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=validation_dataset,\n",
    "#     optimizers=(optimizer, None)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_638393/566792305.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "#Place holder training argument just to load the model\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./trainer_checkpoints',\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    num_train_epochs=3,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:   0%|                                                                                          | 0/2971 [00:00<?, ?it/s]/tmp/ipykernel_638393/2936927997.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_638393/2936927997.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(counter_speech_ids, dtype=torch.long)\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Processing Sentences: 100%|███████████████████████████████████████████████████████████████████████████████| 2971/2971 [01:43<00:00, 28.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference completed for 2971 sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm  # For progress bar\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Create DataLoader with smaller batch size\n",
    "batch_size = 8\n",
    "dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "all_predictions = []\n",
    "all_references = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Get the total number of sentences\n",
    "total_sentences = len(test_dataset)\n",
    "\n",
    "# Batch inference with memory clearing and progress tracking\n",
    "with torch.no_grad():\n",
    "    completed_sentences = 0\n",
    "    with tqdm(total=total_sentences, desc=\"Processing Sentences\") as pbar:\n",
    "        for batch in dataloader:\n",
    "            # Move batch to GPU\n",
    "            batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "\n",
    "            # Perform inference\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            # Move tensors back to CPU immediately to free GPU memory\n",
    "            all_predictions.append(outputs.logits.cpu())\n",
    "            all_references.append(batch['labels'].cpu())\n",
    "\n",
    "            # Update progress bar\n",
    "            batch_size = batch['input_ids'].size(0)\n",
    "            completed_sentences += batch_size\n",
    "            pbar.update(batch_size)\n",
    "\n",
    "            # Force memory release\n",
    "            del batch\n",
    "            del outputs\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "# Print completion message\n",
    "print(f\"Inference completed for {completed_sentences} sentences.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate predictions and references\n",
    "predictions = torch.cat(all_predictions)\n",
    "references = torch.cat(all_references)\n",
    "\n",
    "predictions = predictions.squeeze(1)  # Shape: (total_samples, 128)\n",
    "references = references.squeeze(1)\n",
    "\n",
    "predictions = predictions.argmax(dim=-1)\n",
    "\n",
    "# Decode predictions and references\n",
    "decoded_predictions = test_dataset.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "decoded_references = test_dataset.tokenizer.batch_decode(references, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 0.1297\n",
      "ROUGE-2: 0.0030\n",
      "ROUGE-L: 0.1045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad4916b4cc648d1ae3f9143a41f3892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate ROUGE scores\n",
    "rouge_scores = [scorer.score(ref, pred) for ref, pred in zip(decoded_references, decoded_predictions)]\n",
    "rouge1 = np.mean([score['rouge1'].fmeasure for score in rouge_scores])\n",
    "print(f\"ROUGE-1: {rouge1:.4f}\")\n",
    "rouge2 = np.mean([score['rouge2'].fmeasure for score in rouge_scores])\n",
    "print(f\"ROUGE-2: {rouge2:.4f}\")\n",
    "rougeL = np.mean([score['rougeL'].fmeasure for score in rouge_scores])\n",
    "print(f\"ROUGE-L: {rougeL:.4f}\")\n",
    "\n",
    "# Calculate METEOR score\n",
    "# meteor_score = meteor.compute(predictions=decoded_predictions, references=decoded_references)['meteor']\n",
    "# print(f\"METEOR: {meteor_score:.4f}\")\n",
    "\n",
    "# Calculate BERTScore\n",
    "\n",
    "P, R, F1 = bert_score(cands=decoded_predictions, refs=decoded_references, lang='en', verbose=True)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"BERTScore - P: {P.mean():.4f}, R: {R.mean():.4f}, F1: {F1.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
