{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, Trainer, TrainingArguments, BartForConditionalGeneration\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from torch.optim import Adam\n",
    "from accelerate import Accelerator\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn \n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# meteor = load(\"meteor\")\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('../../Dataset/train.csv')\n",
    "testing_data = pd.read_csv('../../Dataset/test.csv')\n",
    "validation_data = pd.read_csv('../../Dataset/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialoGPTDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/hateBERT\")\n",
    "        self.bart_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "        # Intent label mapping\n",
    "        self.categories = {\n",
    "            'informative': 0,\n",
    "            'questioning': 1,\n",
    "            'denouncing': 2,\n",
    "            'positive': 3\n",
    "        }\n",
    "\n",
    "        # âœ… Create a mapping from hate speech â†’ list of intent labels\n",
    "        self.intent_map = (\n",
    "            data.groupby(\"hatespeech\")[\"csType\"]\n",
    "            .apply(lambda x: [self.categories[t.lower()] for t in x.unique()])\n",
    "            .to_dict()\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # Tokenize hate speech\n",
    "        hate_inputs = self.tokenizer(\n",
    "            row[\"hatespeech\"],\n",
    "            return_tensors='pt',\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        # Tokenize counterspeech\n",
    "        counter_inputs = self.bart_tokenizer(\n",
    "            row[\"counterspeech\"],\n",
    "            return_tensors='pt',\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        intent_id = torch.tensor(self.categories[row[\"csType\"].lower()], dtype=torch.long)\n",
    "        all_intents = self.intent_map[row[\"hatespeech\"]]  # âœ… Look up all intents for this hate speech\n",
    "\n",
    "        return {\n",
    "            'input_ids': hate_inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': hate_inputs['attention_mask'].squeeze(0),\n",
    "            'counter_speech': counter_inputs['input_ids'].squeeze(0),\n",
    "            'intent_id': intent_id,\n",
    "            'raw_text': row[\"hatespeech\"],\n",
    "            'all_intents': all_intents  \n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    counter_speech = torch.stack([item['counter_speech'] for item in batch])\n",
    "    intent_id = torch.stack([item['intent_id'] for item in batch])\n",
    "    all_intents = [item['all_intents'] for item in batch]\n",
    "    raw_inputs = [item['raw_text'] for item in batch]\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'counter_speech': counter_speech,\n",
    "        'intent_id': intent_id,\n",
    "        'all_intents': all_intents,\n",
    "        'raw_inputs':raw_inputs\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, next_input):\n",
    "        super(FeatureEncoder, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained('GroNLP/hateBERT')\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = next_input\n",
    "\n",
    "        self.informative_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim, self.output_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.questioning_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim, self.output_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.denouncing_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim, self.output_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.positive_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim, self.output_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hate_speech_h = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        informative_e = self.informative_head(hate_speech_h)\n",
    "        questioning_e = self.questioning_head(hate_speech_h)\n",
    "        denouncing_e = self.denouncing_head(hate_speech_h)\n",
    "        positive_e = self.positive_head(hate_speech_h)\n",
    "\n",
    "        return informative_e, questioning_e, denouncing_e, positive_e, hate_speech_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionFusion(torch.nn.Module):\n",
    "    def __init__(self, hate_speech_dim, intent_dim, output_dim, num_heads=4):\n",
    "        super(CrossAttentionFusion, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        # Ensure dimensions are divisible by num_heads\n",
    "        self.head_dim = intent_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        # Multi-head projection layers\n",
    "        self.q_proj = torch.nn.Linear(hate_speech_dim, intent_dim)\n",
    "        self.k_proj = torch.nn.Linear(intent_dim, intent_dim)\n",
    "        self.v_proj = torch.nn.Linear(intent_dim, intent_dim)\n",
    "        self.out_proj = torch.nn.Linear(intent_dim, output_dim)\n",
    "        \n",
    "        # Layer normalization and feedforward\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(intent_dim)\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(intent_dim)\n",
    "        self.ffn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(intent_dim, intent_dim * 4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(intent_dim * 4, intent_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, hate_speech_h, intent_e):\n",
    "        \"\"\"\n",
    "        hate_speech_h: hate speech embedding [hate_speech_dim]\n",
    "        intent_e: intent embedding [intent_dim]\n",
    "        \"\"\"\n",
    "        # Add batch dimension if not present\n",
    "        if hate_speech_h.dim() == 1:\n",
    "            hate_speech_h = hate_speech_h.unsqueeze(0)  \n",
    "        if intent_e.dim() == 1:\n",
    "            intent_e = intent_e.unsqueeze(0) \n",
    "            \n",
    "        batch_size = hate_speech_h.shape[0]\n",
    " \n",
    "        q = self.q_proj(hate_speech_h)  \n",
    "        \n",
    "        # Project key and value from intent embedding\n",
    "        k = self.k_proj(intent_e)  # [batch_size, intent_dim]\n",
    "        v = self.v_proj(intent_e)  # [batch_size, intent_dim]\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(batch_size, self.num_heads, self.head_dim)  # [batch_size, num_heads, head_dim]\n",
    "        k = k.view(batch_size, self.num_heads, self.head_dim)  \n",
    "        v = v.view(batch_size, self.num_heads, self.head_dim)  \n",
    "        \n",
    "        # Reshape for attention computation (adding sequence length dimension of 1)\n",
    "        q = q.unsqueeze(2)  # [batch_size, num_heads, 1, head_dim]\n",
    "        k = k.unsqueeze(2)  \n",
    "        v = v.unsqueeze(2)  \n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(q, k.transpose(-1, -2)) * self.scale  # [batch_size, num_heads, 1, 1]\n",
    "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        context = torch.matmul(attention_weights, v)  # [batch_size, num_heads, 1, head_dim]\n",
    "        context = context.squeeze(2).view(batch_size, -1)  # [batch_size, intent_dim]\n",
    "        \n",
    "        # First residual block\n",
    "        norm_context = self.layer_norm1(context + intent_e)\n",
    "        \n",
    "        # FFN block\n",
    "        ffn_output = self.ffn(norm_context)\n",
    "        \n",
    "        # Second residual block\n",
    "        final_output = self.layer_norm2(norm_context + ffn_output)\n",
    "        \n",
    "        # Project to output dimension\n",
    "        output = self.out_proj(final_output)\n",
    "        \n",
    "        # Remove batch dimension if it was added\n",
    "        if hate_speech_h.shape[0] == 1 and intent_e.shape[0] == 1:\n",
    "            output = output.squeeze(0)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CounterSpeechNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, encoder_output, max_length):\n",
    "        super(CounterSpeechNetwork, self).__init__()\n",
    "\n",
    "        self.feature_encoder = FeatureEncoder(input_dim, hidden_dim, encoder_output)\n",
    "\n",
    "        self.informative_decoder = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "        self.questioning_decoder = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "        self.denouncing_decoder = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "        self.positive_decoder = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "\n",
    "        bart_dim = self.informative_decoder.config.d_model\n",
    "        self.informative_fusion = CrossAttentionFusion(hidden_dim, encoder_output, bart_dim)\n",
    "        self.questioning_fusion = CrossAttentionFusion(hidden_dim, encoder_output, bart_dim)\n",
    "        self.denouncing_fusion = CrossAttentionFusion(hidden_dim, encoder_output, bart_dim)\n",
    "        self.positive_fusion = CrossAttentionFusion(hidden_dim, encoder_output, bart_dim)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, intent_id, counter_speech=None):\n",
    "        informative_e, questioning_e, denouncing_e, positive_e, hate_speech_h = self.feature_encoder(input_ids, attention_mask)\n",
    "        batch_size = input_ids.size(0)\n",
    "\n",
    "        fused = torch.zeros(batch_size, 1, self.informative_decoder.config.d_model, device=input_ids.device)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            if intent_id[i] == 0:\n",
    "                # Use cross-attention instead of concatenation\n",
    "                fused[i] = self.informative_fusion(hate_speech_h[i], informative_e[i]).unsqueeze(0)\n",
    "            elif intent_id[i] == 1:\n",
    "                fused[i] = self.questioning_fusion(hate_speech_h[i], questioning_e[i]).unsqueeze(0)\n",
    "            elif intent_id[i] == 2:\n",
    "                fused[i] = self.denouncing_fusion(hate_speech_h[i], denouncing_e[i]).unsqueeze(0)\n",
    "            elif intent_id[i] == 3:\n",
    "                fused[i] = self.positive_fusion(hate_speech_h[i], positive_e[i]).unsqueeze(0)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid intent_id: {intent_id[i]}\")\n",
    "\n",
    "        if counter_speech is not None:\n",
    "            losses = []\n",
    "            for i in range(batch_size):\n",
    "                if intent_id[i] == 0:\n",
    "                    output = self.informative_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0))\n",
    "                elif intent_id[i] == 1:\n",
    "                    output = self.questioning_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0))\n",
    "                elif intent_id[i] == 2:\n",
    "                    output = self.denouncing_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0))\n",
    "                elif intent_id[i] == 3:\n",
    "                    output = self.positive_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0))\n",
    "                losses.append(output.loss)\n",
    "            avg_loss = sum(losses) / len(losses)  # Average loss across the batch\n",
    "            return None, avg_loss  # No decoded text during training\n",
    "        else:\n",
    "            decoded_texts = []\n",
    "            for i in range(batch_size):\n",
    "                if intent_id[i] == 0:\n",
    "                    output = self.informative_decoder.generate(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), max_length=self.max_length, num_beams=4, early_stopping=True)\n",
    "                elif intent_id[i] == 1:\n",
    "                    output = self.questioning_decoder.generate(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), max_length=self.max_length, num_beams=4, early_stopping=True)\n",
    "                elif intent_id[i] == 2:\n",
    "                    output = self.denouncing_decoder.generate(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), max_length=self.max_length, num_beams=4, early_stopping=True)\n",
    "                elif intent_id[i] == 3:\n",
    "                    output = self.positive_decoder.generate(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), max_length=self.max_length, num_beams=4, early_stopping=True)\n",
    "                decoded_texts.append(self.tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "            return decoded_texts, None  # Decoded text during inference, no loss\n",
    "        \n",
    "    def judge_responses(self,input_ids, attention_mask,counter_speech=None):\n",
    "        \n",
    "        informative_e, questioning_e, denouncing_e, positive_e, hate_speech_h = self.feature_encoder(input_ids, attention_mask)\n",
    "\n",
    "        batch_size = input_ids.size(0)\n",
    "        output_dict = {'informative':[],'questioning':[],'denouncing':[],'positive':[]}\n",
    "        informative_fused = torch.zeros(batch_size, 1, self.informative_decoder.config.d_model, device=input_ids.device)\n",
    "        questioning_fused = torch.zeros(batch_size, 1, self.informative_decoder.config.d_model, device=input_ids.device)\n",
    "        denouncing_fused = torch.zeros(batch_size, 1, self.informative_decoder.config.d_model, device=input_ids.device)\n",
    "        positive_fused = torch.zeros(batch_size, 1, self.informative_decoder.config.d_model, device=input_ids.device)\n",
    "        \n",
    "    \n",
    "        for i in range(batch_size):\n",
    "            informative_fused[i] = self.informative_fusion( hate_speech_h[i], informative_e[i]).unsqueeze(0)\n",
    "            questioning_fused[i] = self.questioning_fusion( hate_speech_h[i], questioning_e[i]).unsqueeze(0)\n",
    "            denouncing_fused[i] = self.denouncing_fusion(hate_speech_h[i], denouncing_e[i]).unsqueeze(0)\n",
    "            positive_fused[i] = self.positive_fusion(hate_speech_h[i] , positive_e[i]).unsqueeze(0)\n",
    "\n",
    "\n",
    "            output_dict['informative'].append(self.informative_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=informative_fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0)))\n",
    "            output_dict['questioning'].append(self.questioning_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=questioning_fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0)))\n",
    "            output_dict['denouncing'].append(self.denouncing_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=denouncing_fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0)))\n",
    "            output_dict['positive'].append(self.positive_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=positive_fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0)))\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DialoGPTDataset(testing_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_judge_model(model, test_dataloader, device):\n",
    "    categories = {\n",
    "        'informative': 0,\n",
    "        'questioning': 1,\n",
    "        'denouncing': 2,\n",
    "        'positive': 3\n",
    "    }\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    judge_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "    judge_model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\").to(device)\n",
    "    judge_model.eval()\n",
    "    print(\"Judge model ready!\")\n",
    "\n",
    "    total_samples = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    test_loop = tqdm(test_dataloader, desc=\"Test Evaluation\", leave=True)\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            counter_speech = batch['counter_speech'].to(device)\n",
    "            raw_inputs = batch['raw_inputs']\n",
    "            all_intents = batch['all_intents']  # list of gold intents per sample (as int indices)\n",
    "\n",
    "            outputs = model.judge_responses(input_ids, attention_mask, counter_speech)\n",
    "\n",
    "            for i in range(len(input_ids)):\n",
    "                intent_scores = {}\n",
    "\n",
    "                for intent in [\"informative\", \"questioning\", \"denouncing\", \"positive\"]:\n",
    "                    response = model.tokenizer.decode(\n",
    "                        torch.argmax(outputs[intent][i].logits, dim=-1)[0],\n",
    "                        skip_special_tokens=True\n",
    "                    )\n",
    "\n",
    "                    # Prompting for a rating\n",
    "                    prompt = (\n",
    "                        f\"Hate speech: {raw_inputs[i]}\\n\"\n",
    "                        f\"Proposed counterspeech (intent: {intent}): {response}\\n\\n\"\n",
    "                        f\"On a scale of 1 to 10, how appropriate and effective is this counterspeech in response to the hate speech? Just respond with a number.\"\n",
    "                    )\n",
    "\n",
    "                    judge_input = judge_tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "                    output_ids = judge_model.generate(judge_input, max_new_tokens=10, pad_token_id=judge_tokenizer.eos_token_id)\n",
    "                    score_text = judge_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                    try:\n",
    "                        # Extract the first number in response (robust to \"I would give it a 7\" etc.)\n",
    "                        score = next((float(s) for s in score_text.split() if s.replace('.', '', 1).isdigit()), 0)\n",
    "                        score = max(0, min(score, 10))  # Clamp between 0 and 10\n",
    "                    except:\n",
    "                        score = 0  # Fallback score if judge LM fails\n",
    "\n",
    "                    intent_scores[intent] = score\n",
    "\n",
    "                # Pick best scoring intent\n",
    "                best_intent = max(intent_scores, key=intent_scores.get)\n",
    "                best_intent_idx = categories[best_intent]\n",
    "\n",
    "                if best_intent_idx in all_intents[i]:\n",
    "                    correct_predictions += 1\n",
    "                total_samples += 1\n",
    "\n",
    "            test_loop.set_postfix({'accuracy': correct_predictions / total_samples if total_samples else 0})\n",
    "\n",
    "    final_accuracy = correct_predictions / total_samples if total_samples else 0\n",
    "    return final_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required data for METEOR\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize model\n",
    "model = CounterSpeechNetwork(input_dim=128, hidden_dim=768, encoder_output=256, max_length=50)\n",
    "model.load_state_dict(torch.load(\"HateBERT_cross_attention_final.pth\", map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Evaluation loop\n",
    "test_predictions = []\n",
    "test_references = []\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "meteor_scores = []\n",
    "cosine_sims = []\n",
    "\n",
    "test_loop = tqdm(test_dataloader, desc=\"Test Evaluation\", leave=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        counter_speech = batch['counter_speech'].to(device)  # Reference texts\n",
    "        intent_ids = batch['intent_id'].to(device)\n",
    "\n",
    "        predictions, logits = model(input_ids, attention_mask, intent_ids)\n",
    "        \n",
    "        # Decode predictions and references\n",
    "        pred_texts = predictions\n",
    "        ref_texts = [model.tokenizer.decode(cs, skip_special_tokens=True) for cs in counter_speech]\n",
    "\n",
    "        test_predictions.extend(pred_texts)\n",
    "        test_references.extend(ref_texts)\n",
    "\n",
    "        # Compute METEOR and Cosine Similarity\n",
    "        for pred, ref in zip(pred_texts, ref_texts):\n",
    "            score = meteor_score([ref.split()], pred.split())\n",
    "            meteor_scores.append(score)\n",
    "\n",
    "            # Cosine similarity using simple TF representation\n",
    "            pred_vec = model.tokenizer(pred, return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].float()\n",
    "            ref_vec = model.tokenizer(ref, return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].float()\n",
    "\n",
    "            pred_vec = normalize(torch.sum(pred_vec, dim=1).numpy().reshape(1, -1))\n",
    "            ref_vec = normalize(torch.sum(ref_vec, dim=1).numpy().reshape(1, -1))\n",
    "            cos_sim = cosine_similarity(pred_vec, ref_vec)[0][0]\n",
    "            cosine_sims.append(cos_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BERTScore\n",
    "P, R, F1 = bert_score(test_predictions, test_references, lang=\"en\", verbose=True)\n",
    "\n",
    "# Compute ROUGE\n",
    "rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "for pred, ref in zip(test_predictions, test_references):\n",
    "    scores = scorer.score(ref, pred)\n",
    "    rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "    rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "    rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "avg_rouge1 = sum(rouge_scores['rouge1']) / len(rouge_scores['rouge1'])\n",
    "avg_rouge2 = sum(rouge_scores['rouge2']) / len(rouge_scores['rouge2'])\n",
    "avg_rougeL = sum(rouge_scores['rougeL']) / len(rouge_scores['rougeL'])\n",
    "avg_meteor = sum(meteor_scores) / len(meteor_scores)\n",
    "avg_cosine = sum(cosine_sims) / len(cosine_sims)\n",
    "\n",
    "# Compute category (intent) accuracy\n",
    "intent_accuracy = evaluate_with_judge_model(model,test_dataloader,device)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n=== Evaluation Metrics ===\")\n",
    "print(f\"Total Predictions: {len(test_predictions)}\")\n",
    "print(f\"BERTScore - Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")\n",
    "print(f\"ROUGE - Rouge-1: {avg_rouge1:.4f}, Rouge-2: {avg_rouge2:.4f}, Rouge-L: {avg_rougeL:.4f}\")\n",
    "print(f\"METEOR: {avg_meteor:.4f}\")\n",
    "print(f\"Cosine Similarity: {avg_cosine:.4f}\")\n",
    "print(f\"Category Accuracy (Intent): {intent_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test predictions to a text file\n",
    "model_name = \"HateBERT-cross-attention\"\n",
    "txt_filename = f\"predictions_{model_name}.txt\"\n",
    "with open(txt_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    for pred in test_predictions:\n",
    "        f.write(pred.strip() + \"\\n\")\n",
    "\n",
    "print(f\"ðŸ“„ Saved predictions to {txt_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
