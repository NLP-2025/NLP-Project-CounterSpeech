{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2ba602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, Trainer, TrainingArguments, BartForConditionalGeneration\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from torch.optim import Adam\n",
    "from accelerate import Accelerator\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn \n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e2bea0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 14 16:56:40 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A5000               Off | 00000000:01:00.0 Off |                  Off |\n",
      "| 30%   26C    P8              15W / 230W |      3MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe89323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# meteor = load(\"meteor\")\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36a97a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/dhruv/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnoel22338\u001b[0m (\u001b[33mnlp_project_team\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login --relogin --verify f59d448beb3315f3efbc5a0a80d9d2c346926308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13ba515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('../Dataset/train.csv')\n",
    "testing_data = pd.read_csv('../Dataset/test.csv')\n",
    "validation_data = pd.read_csv('../Dataset/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf54c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['hatespeech', 'csType', 'counterspeech', 'Suggest', 'Relevance',\n",
      "       'Aggressive', 'Complexity', 'Comments', 'source', 'claim',\n",
      "       'centralTopic', 'speakerIntent', 'targetGroup', 'relevantPowerDynamics',\n",
      "       'hatespeechImplication', 'targetGroupEmotionalReaction',\n",
      "       'targetGroupCognitiveReaction', 'hatespeechOffensiveness', 'id',\n",
      "       'is_high_quality', 'hs_id', 'hatespeechTarget', 'powerDynamics',\n",
      "       'prompt_offensiveness', 'prompt_target_group', 'prompt_speaker_intent',\n",
      "       'prompt_power_dynamics', 'prompt_implication',\n",
      "       'prompt_emotional_reaction', 'prompt_cognitive_reaction',\n",
      "       'prompt_cs_generation'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "columns = training_data.columns\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af0e3b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialoGPTDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/hateBERT\")\n",
    "        self.bart_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "        # Intent label mapping\n",
    "        self.categories = {\n",
    "            'informative': 0,\n",
    "            'questioning': 1,\n",
    "            'denouncing': 2,\n",
    "            'positive': 3\n",
    "        }\n",
    "\n",
    "        # Create a mapping from hate speech â†’ list of intent labels\n",
    "        self.intent_map = (\n",
    "            data.groupby(\"hatespeech\")[\"csType\"]\n",
    "            .apply(lambda x: [self.categories[t.lower()] for t in x.unique()])\n",
    "            .to_dict()\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # Tokenize hate speech\n",
    "        hate_inputs = self.tokenizer(\n",
    "            row[\"hatespeech\"],\n",
    "            return_tensors='pt',\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        # Tokenize counterspeech\n",
    "        counter_inputs = self.bart_tokenizer(\n",
    "            row[\"counterspeech\"],\n",
    "            return_tensors='pt',\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        intent_id = torch.tensor(self.categories[row[\"csType\"].lower()], dtype=torch.long)\n",
    "        all_intents = self.intent_map[row[\"hatespeech\"]]  #Look up all intents for this hate speech\n",
    "\n",
    "        return {\n",
    "            'input_ids': hate_inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': hate_inputs['attention_mask'].squeeze(0),\n",
    "            'counter_speech': counter_inputs['input_ids'].squeeze(0),\n",
    "            'intent_id': intent_id,\n",
    "            'raw_text': row[\"hatespeech\"],\n",
    "            'all_intents': all_intents  \n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    counter_speech = torch.stack([item['counter_speech'] for item in batch])\n",
    "    intent_id = torch.stack([item['intent_id'] for item in batch])\n",
    "    all_intents = [item['all_intents'] for item in batch]\n",
    "    raw_inputs = [item['raw_text'] for item in batch]\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'counter_speech': counter_speech,\n",
    "        'intent_id': intent_id,\n",
    "        'all_intents': all_intents,\n",
    "        'raw_inputs':raw_inputs\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b35316d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9532\n",
      "2971\n",
      "1470\n",
      "{'input_ids': tensor([  101,  2672,  1996,  4895,  2071,  2831,  2000,  2216,  4004,  1998,\n",
      "         3060,  3741,  3625,  2005,  3938,  1009,  1997,  1996, 10796,  1999,\n",
      "         1996, 17401,  2612,  1997, 22604,  2006,  2023, 14636,  2055,  4785,\n",
      "         2689,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'counter_speech': tensor([    0,   133,   201,    16,     5,   200,   144,  2902, 15024,   247,\n",
      "           11,     5,   232,   111,    25,     5,   232,    17,    27,    29,\n",
      "          934,  2683,     8,  1861,   476,     4,    52,    32,    70,     7,\n",
      "         4887,   259,     8,   240,     7,   173,   865,    11,   865,     7,\n",
      "         1045,  5068,   464,     6,    25,  4340,     7,  7547,     5,  8411,\n",
      "            7,   643,     4,     2,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1]), 'intent_id': tensor(0), 'raw_text': 'Maybe the UN could talk to those asian and african nations responsible for 90+ of the pollution in the oceans instead of insisting on this bullshit about climate change.', 'all_intents': [0, 1, 2, 3]}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DialoGPTDataset(training_data)\n",
    "test_dataset = DialoGPTDataset(testing_data)\n",
    "validation_dataset = DialoGPTDataset(validation_data)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(validation_dataset))\n",
    "\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "795e9f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, next_input):\n",
    "        super(FeatureEncoder, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained('GroNLP/hateBERT')\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = next_input\n",
    "\n",
    "        self.informative_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim, self.output_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.questioning_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim, self.output_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.denouncing_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim, self.output_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.positive_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim, self.output_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hate_speech_h = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        informative_e = self.informative_head(hate_speech_h)\n",
    "        questioning_e = self.questioning_head(hate_speech_h)\n",
    "        denouncing_e = self.denouncing_head(hate_speech_h)\n",
    "        positive_e = self.positive_head(hate_speech_h)\n",
    "\n",
    "        return informative_e, questioning_e, denouncing_e, positive_e, hate_speech_h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a46dbb3",
   "metadata": {},
   "source": [
    "### Linear Fusion Mechanism in CounterSpeechNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d49cc606",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CounterSpeechNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, encoder_output, max_length):\n",
    "        super(CounterSpeechNetwork, self).__init__()\n",
    "\n",
    "        self.feature_encoder = FeatureEncoder(input_dim, hidden_dim, encoder_output)\n",
    "\n",
    "        self.informative_decoder = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "        self.questioning_decoder = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "        self.denouncing_decoder = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "        self.positive_decoder = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "\n",
    "        self.informative_fusion = torch.nn.Linear(hidden_dim + encoder_output, self.informative_decoder.config.d_model)\n",
    "        self.questioning_fusion = torch.nn.Linear(hidden_dim + encoder_output, self.questioning_decoder.config.d_model)\n",
    "        self.denouncing_fusion = torch.nn.Linear(hidden_dim + encoder_output, self.denouncing_decoder.config.d_model)\n",
    "        self.positive_fusion = torch.nn.Linear(hidden_dim + encoder_output, self.positive_decoder.config.d_model)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, intent_id, counter_speech=None):\n",
    "        informative_e, questioning_e, denouncing_e, positive_e, hate_speech_h = self.feature_encoder(input_ids, attention_mask)\n",
    "\n",
    "        batch_size = input_ids.size(0)\n",
    "\n",
    "        fused = torch.zeros(batch_size, 1, self.informative_decoder.config.d_model, device=input_ids.device)\n",
    "    \n",
    "        for i in range(batch_size):\n",
    "            if intent_id[i] == 0:\n",
    "                fused[i] = self.informative_fusion(torch.cat((hate_speech_h[i], informative_e[i]), dim=-1)).unsqueeze(0)\n",
    "            elif intent_id[i] == 1:\n",
    "                fused[i] = self.questioning_fusion(torch.cat((hate_speech_h[i], questioning_e[i]), dim=-1)).unsqueeze(0)\n",
    "            elif intent_id[i] == 2:\n",
    "                fused[i] = self.denouncing_fusion(torch.cat((hate_speech_h[i], denouncing_e[i]), dim=-1)).unsqueeze(0)\n",
    "            elif intent_id[i] == 3:\n",
    "                fused[i] = self.positive_fusion(torch.cat((hate_speech_h[i], positive_e[i]), dim=-1)).unsqueeze(0)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid intent_id: {intent_id[i]}\")\n",
    "\n",
    "        if counter_speech is not None:\n",
    "            losses = []\n",
    "            for i in range(batch_size):\n",
    "                if intent_id[i] == 0:\n",
    "                    output = self.informative_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0))\n",
    "                elif intent_id[i] == 1:\n",
    "                    output = self.questioning_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0))\n",
    "                elif intent_id[i] == 2:\n",
    "                    output = self.denouncing_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0))\n",
    "                elif intent_id[i] == 3:\n",
    "                    output = self.positive_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0))\n",
    "                losses.append(output.loss)\n",
    "            avg_loss = sum(losses) / len(losses)  # Average loss across the batch\n",
    "            return None, avg_loss  # No decoded text during training\n",
    "        else:\n",
    "            decoded_texts = []\n",
    "            for i in range(batch_size):\n",
    "                if intent_id[i] == 0:\n",
    "                    output = self.informative_decoder.generate(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), max_length=self.max_length, num_beams=4, early_stopping=True)\n",
    "                elif intent_id[i] == 1:\n",
    "                    output = self.questioning_decoder.generate(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), max_length=self.max_length, num_beams=4, early_stopping=True)\n",
    "                elif intent_id[i] == 2:\n",
    "                    output = self.denouncing_decoder.generate(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), max_length=self.max_length, num_beams=4, early_stopping=True)\n",
    "                elif intent_id[i] == 3:\n",
    "                    output = self.positive_decoder.generate(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), max_length=self.max_length, num_beams=4, early_stopping=True)\n",
    "                decoded_texts.append(self.tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "            return decoded_texts, None  # Decoded text during inference, no loss\n",
    "    def judge_responses(self,input_ids, attention_mask,counter_speech=None):\n",
    "        \n",
    "        informative_e, questioning_e, denouncing_e, positive_e, hate_speech_h = self.feature_encoder(input_ids, attention_mask)\n",
    "\n",
    "        batch_size = input_ids.size(0)\n",
    "        output_dict = {'informative':[],'questioning':[],'denouncing':[],'positive':[]}\n",
    "        informative_fused = torch.zeros(batch_size, 1, self.informative_decoder.config.d_model, device=input_ids.device)\n",
    "        questioning_fused = torch.zeros(batch_size, 1, self.informative_decoder.config.d_model, device=input_ids.device)\n",
    "        denouncing_fused = torch.zeros(batch_size, 1, self.informative_decoder.config.d_model, device=input_ids.device)\n",
    "        positive_fused = torch.zeros(batch_size, 1, self.informative_decoder.config.d_model, device=input_ids.device)\n",
    "        \n",
    "    \n",
    "        for i in range(batch_size):\n",
    "            informative_fused[i] = self.informative_fusion(torch.cat((hate_speech_h[i], informative_e[i]), dim=-1)).unsqueeze(0)\n",
    "            questioning_fused[i] = self.questioning_fusion(torch.cat((hate_speech_h[i], questioning_e[i]), dim=-1)).unsqueeze(0)\n",
    "            denouncing_fused[i] = self.denouncing_fusion(torch.cat((hate_speech_h[i], denouncing_e[i]), dim=-1)).unsqueeze(0)\n",
    "            positive_fused[i] = self.positive_fusion(torch.cat((hate_speech_h[i], positive_e[i]), dim=-1)).unsqueeze(0)\n",
    "\n",
    "\n",
    "            output_dict['informative'].append(self.informative_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=informative_fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0)))\n",
    "            output_dict['questioning'].append(self.questioning_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=questioning_fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0)))\n",
    "            output_dict['denouncing'].append(self.denouncing_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=denouncing_fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0)))\n",
    "            output_dict['positive'].append(self.positive_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=positive_fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0)))\n",
    "\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47c15eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 2.1134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Validation Loss: 0.7529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train Loss: 0.7974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Validation Loss: 0.7116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train Loss: 0.7171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Validation Loss: 0.6950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train Loss: 0.6578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Validation Loss: 0.6852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train Loss: 0.6061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Validation Loss: 0.6786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train Loss: 0.5604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Validation Loss: 0.6788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train Loss: 0.5169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Validation Loss: 0.6864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Train Loss: 0.4765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Validation Loss: 0.6965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Train Loss: 0.4376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Validation Loss: 0.7042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Train Loss: 0.4006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Validation Loss: 0.7238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "model = CounterSpeechNetwork(input_dim=128, hidden_dim=768, encoder_output=256, max_length=50)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn= custom_collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    # Wrap train_dataloader with tqdm for training progress\n",
    "    train_loop = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs} [Train]\", leave=False)\n",
    "    for batch in train_loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        counter_speech = batch['counter_speech'].to(device)\n",
    "        intent_ids = batch['intent_id'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Process the entire batch at once\n",
    "        _, loss = model(input_ids, attention_mask, intent_ids, counter_speech)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Update tqdm with current batch loss\n",
    "        train_loop.set_postfix({'batch_loss': loss.item(), 'avg_loss': total_train_loss / (train_loop.n + 1)})\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} | Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    # Wrap validation_dataloader with tqdm for validation progress\n",
    "    val_loop = tqdm(validation_dataloader, desc=f\"Epoch {epoch + 1}/{epochs} [Validation]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            counter_speech = batch['counter_speech'].to(device)\n",
    "            intent_ids = batch['intent_id'].to(device)\n",
    "\n",
    "            # Process the entire batch at once\n",
    "            _, loss = model(input_ids, attention_mask, intent_ids, counter_speech)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            # Update tqdm with current batch loss\n",
    "            val_loop.set_postfix({'batch_loss': loss.item(), 'avg_loss': total_val_loss / (val_loop.n + 1)})\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} | Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ca125a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'HateBERT_linear_fusion_final.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d076f3",
   "metadata": {},
   "source": [
    "### Evaluation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e437f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DialoGPTDataset(testing_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d755976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_judge_model(model, test_dataloader, device):\n",
    "    categories = {\n",
    "        'informative': 0,\n",
    "        'questioning': 1,\n",
    "        'denouncing': 2,\n",
    "        'positive': 3\n",
    "    }\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    judge_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "    judge_model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\").to(device)\n",
    "    judge_model.eval()\n",
    "    print(\"Judge model ready!\")\n",
    "\n",
    "    total_samples = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    test_loop = tqdm(test_dataloader, desc=\"Test Evaluation\", leave=True)\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            counter_speech = batch['counter_speech'].to(device)\n",
    "            raw_inputs = batch['raw_inputs']\n",
    "            all_intents = batch['all_intents']  # list of gold intents per sample (as int indices)\n",
    "\n",
    "            outputs = model.judge_responses(input_ids, attention_mask, counter_speech)\n",
    "\n",
    "            for i in range(len(input_ids)):\n",
    "                intent_scores = {}\n",
    "\n",
    "                for intent in [\"informative\", \"questioning\", \"denouncing\", \"positive\"]:\n",
    "                    response = model.tokenizer.decode(\n",
    "                        torch.argmax(outputs[intent][i].logits, dim=-1)[0],\n",
    "                        skip_special_tokens=True\n",
    "                    )\n",
    "\n",
    "                    # Prompting for a rating\n",
    "                    prompt = (\n",
    "                        f\"Hate speech: {raw_inputs[i]}\\n\"\n",
    "                        f\"Proposed counterspeech (intent: {intent}): {response}\\n\\n\"\n",
    "                        f\"On a scale of 1 to 10, how appropriate and effective is this counterspeech in response to the hate speech? Just respond with a number.\"\n",
    "                    )\n",
    "\n",
    "                    judge_input = judge_tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "                    output_ids = judge_model.generate(judge_input, max_new_tokens=10, pad_token_id=judge_tokenizer.eos_token_id)\n",
    "                    score_text = judge_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                    try:\n",
    "                        # Extract the first number in response (robust to \"I would give it a 7\" etc.)\n",
    "                        score = next((float(s) for s in score_text.split() if s.replace('.', '', 1).isdigit()), 0)\n",
    "                        score = max(0, min(score, 10))  # Clamp between 0 and 10\n",
    "                    except:\n",
    "                        score = 0  # Fallback score if judge LM fails\n",
    "\n",
    "                    intent_scores[intent] = score\n",
    "\n",
    "                # Pick best scoring intent\n",
    "                best_intent = max(intent_scores, key=intent_scores.get)\n",
    "                best_intent_idx = categories[best_intent]\n",
    "\n",
    "                if best_intent_idx in all_intents[i]:\n",
    "                    correct_predictions += 1\n",
    "                total_samples += 1\n",
    "\n",
    "            test_loop.set_postfix({'accuracy': correct_predictions / total_samples if total_samples else 0})\n",
    "\n",
    "    final_accuracy = correct_predictions / total_samples if total_samples else 0\n",
    "    return final_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a83c412f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1583725/3338225103.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"HateBERT_linear_fusion_final.pth\", map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
      "Test Evaluation:   0%|                                                                                                                                                       | 0/93 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Test Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [10:13<00:00,  6.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# # Download required data for METEOR\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Initialize model\n",
    "model = CounterSpeechNetwork(input_dim=128, hidden_dim=768, encoder_output=256, max_length=50)\n",
    "model.load_state_dict(torch.load(\"HateBERT_linear_fusion_final.pth\", map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Evaluation loop\n",
    "test_predictions = []\n",
    "test_references = []\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "meteor_scores = []\n",
    "cosine_sims = []\n",
    "\n",
    "test_loop = tqdm(test_dataloader, desc=\"Test Evaluation\", leave=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        counter_speech = batch['counter_speech'].to(device)  # Reference texts\n",
    "        intent_ids = batch['intent_id'].to(device)\n",
    "\n",
    "        predictions, logits = model(input_ids, attention_mask, intent_ids)\n",
    "        \n",
    "        # Decode predictions and references\n",
    "        pred_texts = predictions\n",
    "        ref_texts = [model.tokenizer.decode(cs, skip_special_tokens=True) for cs in counter_speech]\n",
    "\n",
    "        test_predictions.extend(pred_texts)\n",
    "        test_references.extend(ref_texts)\n",
    "\n",
    "        # Compute METEOR and Cosine Similarity\n",
    "        for pred, ref in zip(pred_texts, ref_texts):\n",
    "            score = meteor_score([ref.split()], pred.split())\n",
    "            meteor_scores.append(score)\n",
    "\n",
    "            # Cosine similarity using simple TF representation\n",
    "            pred_vec = model.tokenizer(pred, return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].float()\n",
    "            ref_vec = model.tokenizer(ref, return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].float()\n",
    "\n",
    "            pred_vec = normalize(torch.sum(pred_vec, dim=1).numpy().reshape(1, -1))\n",
    "            ref_vec = normalize(torch.sum(ref_vec, dim=1).numpy().reshape(1, -1))\n",
    "            cos_sim = cosine_similarity(pred_vec, ref_vec)[0][0]\n",
    "            cosine_sims.append(cos_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67382e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3286d6d8e285450ba1e0c926041b37fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0396b24b6c0a498c93d743815f78b551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 10.16 seconds, 292.32 sentences/sec\n",
      "Judge model ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Evaluation:   0%|                                                                                                                                                       | 0/93 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "# Compute BERTScore\n",
    "P, R, F1 = bert_score(test_predictions, test_references, lang=\"en\", verbose=True)\n",
    "\n",
    "# Compute ROUGE\n",
    "rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "for pred, ref in zip(test_predictions, test_references):\n",
    "    scores = scorer.score(ref, pred)\n",
    "    rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "    rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "    rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "avg_rouge1 = sum(rouge_scores['rouge1']) / len(rouge_scores['rouge1'])\n",
    "avg_rouge2 = sum(rouge_scores['rouge2']) / len(rouge_scores['rouge2'])\n",
    "avg_rougeL = sum(rouge_scores['rougeL']) / len(rouge_scores['rougeL'])\n",
    "avg_meteor = sum(meteor_scores) / len(meteor_scores)\n",
    "avg_cosine = sum(cosine_sims) / len(cosine_sims)\n",
    "\n",
    "# Compute category (intent) accuracy\n",
    "intent_accuracy = evaluate_with_judge_model(model,test_dataloader,device)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n=== Evaluation Metrics ===\")\n",
    "print(f\"Total Predictions: {len(test_predictions)}\")\n",
    "print(f\"BERTScore - Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")\n",
    "print(f\"ROUGE - Rouge-1: {avg_rouge1:.4f}, Rouge-2: {avg_rouge2:.4f}, Rouge-L: {avg_rougeL:.4f}\")\n",
    "print(f\"METEOR: {avg_meteor:.4f}\")\n",
    "print(f\"Cosine Similarity: {avg_cosine:.4f}\")\n",
    "print(f\"Category Accuracy (Intent): {intent_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3722cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test predictions to a text file\n",
    "model_name = \"HateBERT-linear-fusion\"\n",
    "txt_filename = f\"predictions_{model_name}.txt\"\n",
    "with open(txt_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    for pred in test_predictions:\n",
    "        f.write(pred.strip() + \"\\n\")\n",
    "\n",
    "print(f\"ðŸ“„ Saved predictions to {txt_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c52988-cd9e-44df-83ef-e350cedc33d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Predictions: 2971\n",
    "# BERTScore - Precision: 0.8688, Recall: 0.8705, F1: 0.8696\n",
    "# ROUGE - Rouge-1: 0.2504, Rouge-2: 0.0644, Rouge-L: 0.1752\n",
    "# METEOR: 0.1543\n",
    "# Cosine Similarity: 1.0000\n",
    "# Category Accuracy (Intent): 0.7519"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_project)",
   "language": "python",
   "name": "nlp_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
