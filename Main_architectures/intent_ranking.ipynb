{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QJ-yxJ6P_6dU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, Trainer, TrainingArguments, BartForConditionalGeneration\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from torch.optim import Adam\n",
    "from accelerate import Accelerator\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239,
     "referenced_widgets": [
      "36aff3cc8ee845b08abd7f18589726fa",
      "f5df75b26d1d464fb722dff990b5476a",
      "00606f57d8884a8da09c02a2b181667c",
      "5ae640fb971841d9b86737e6fd0b4c78",
      "452dc6a25a544daab3217fae74b699e9",
      "fccb872427ac46018921c52b405210ea",
      "1274c2be26b64ab3b4ac358f7606a310",
      "baa6a137ee63491cb9a5f54e44095154",
      "13cfef767914493fb7c16b42845c4c45",
      "95abeb3513704f59b66711a516c6130a",
      "4d09db8336c340e4be9bc537537f4b0d"
     ]
    },
    "id": "vTDocM7nI_Hw",
    "outputId": "f3d7a035-b43f-46dc-d8e4-166f70fc7b90"
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# meteor = load(\"meteor\")\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eLC8QX4qI_Hw"
   },
   "outputs": [],
   "source": [
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hj0qY8zOI_Hx",
    "outputId": "087c651e-4068-420a-a242-4f2ba44c48ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/dhruv/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnoel22338\u001b[0m (\u001b[33mnlp_project_team\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login --relogin --verify f59d448beb3315f3efbc5a0a80d9d2c346926308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "B6MBEOJg_6dV"
   },
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('../Dataset/train.csv')\n",
    "testing_data = pd.read_csv('../Dataset/test.csv')\n",
    "validation_data = pd.read_csv('../Dataset/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 865
    },
    "id": "KPdXrxQg_6dW",
    "outputId": "da88817b-93ac-4215-bfa8-7c8849baf2dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          hatespeech       csType  \\\n",
      "0  Maybe the UN could talk to those asian and afr...  Informative   \n",
      "1  Maybe the UN could talk to those asian and afr...  Questioning   \n",
      "2  Maybe the UN could talk to those asian and afr...   Denouncing   \n",
      "3  Maybe the UN could talk to those asian and afr...     Positive   \n",
      "4  Juice Jews are worse that nukes. Caption to a ...  Informative   \n",
      "\n",
      "                                       counterspeech Suggest  Relevance  \\\n",
      "0  The us is the second most polluting country in...       3        4.0   \n",
      "1  Doesn't everyone on the planet have a responsi...                3.0   \n",
      "2  The world would be a better place if people we...       1        1.0   \n",
      "3  You're right, ocean pollution is one of the ma...       3        4.0   \n",
      "4  Anti-semitism is a serious problem that we nee...       2        3.0   \n",
      "\n",
      "   Aggressive  Complexity  Comments source  \\\n",
      "0         2.0         3.0       NaN  Human   \n",
      "1         2.0         2.0       NaN  Human   \n",
      "2         2.0         2.0       NaN  Human   \n",
      "3         2.0         2.0       NaN  Human   \n",
      "4         2.0         2.0       NaN  Human   \n",
      "\n",
      "                                               claim  ... hatespeechTarget  \\\n",
      "0  The UN should focus on Asian and African natio...  ...     asian_people   \n",
      "1  The UN should focus on Asian and African natio...  ...     asian_people   \n",
      "2  The UN should focus on Asian and African natio...  ...     asian_people   \n",
      "3  The UN should focus on Asian and African natio...  ...     asian_people   \n",
      "4                         Jews are worse than nukes.  ...             jews   \n",
      "\n",
      "  powerDynamics                               prompt_offensiveness  \\\n",
      "0        racial  Analyze the offensiveness of the statement: Ma...   \n",
      "1        racial  Analyze the offensiveness of the statement: Ma...   \n",
      "2        racial  Analyze the offensiveness of the statement: Ma...   \n",
      "3        racial  Analyze the offensiveness of the statement: Ma...   \n",
      "4      religion  Analyze the offensiveness of the statement: Ju...   \n",
      "\n",
      "                                 prompt_target_group  \\\n",
      "0  Identify the group of people that the speaker ...   \n",
      "1  Identify the group of people that the speaker ...   \n",
      "2  Identify the group of people that the speaker ...   \n",
      "3  Identify the group of people that the speaker ...   \n",
      "4  Identify the group of people that the speaker ...   \n",
      "\n",
      "                               prompt_speaker_intent  \\\n",
      "0  Analyze the speaker's intention behind writing...   \n",
      "1  Analyze the speaker's intention behind writing...   \n",
      "2  Analyze the speaker's intention behind writing...   \n",
      "3  Analyze the speaker's intention behind writing...   \n",
      "4  Analyze the speaker's intention behind writing...   \n",
      "\n",
      "                               prompt_power_dynamics  \\\n",
      "0  Explain the underlying power dynamics between ...   \n",
      "1  Explain the underlying power dynamics between ...   \n",
      "2  Explain the underlying power dynamics between ...   \n",
      "3  Explain the underlying power dynamics between ...   \n",
      "4  Explain the underlying power dynamics between ...   \n",
      "\n",
      "                                  prompt_implication  \\\n",
      "0  Explain the implied meaning underlying the off...   \n",
      "1  Explain the implied meaning underlying the off...   \n",
      "2  Explain the implied meaning underlying the off...   \n",
      "3  Explain the implied meaning underlying the off...   \n",
      "4  Explain the implied meaning underlying the off...   \n",
      "\n",
      "                           prompt_emotional_reaction  \\\n",
      "0  Describe how the target group might feel emoti...   \n",
      "1  Describe how the target group might feel emoti...   \n",
      "2  Describe how the target group might feel emoti...   \n",
      "3  Describe how the target group might feel emoti...   \n",
      "4  Describe how the target group might feel emoti...   \n",
      "\n",
      "                           prompt_cognitive_reaction  \\\n",
      "0  Describe how the target group might react cogn...   \n",
      "1  Describe how the target group might react cogn...   \n",
      "2  Describe how the target group might react cogn...   \n",
      "3  Describe how the target group might react cogn...   \n",
      "4  Describe how the target group might react cogn...   \n",
      "\n",
      "                                prompt_cs_generation  \n",
      "0  Analyze the different aspects such as offensiv...  \n",
      "1  Analyze the different aspects such as offensiv...  \n",
      "2  Analyze the different aspects such as offensiv...  \n",
      "3  Analyze the different aspects such as offensiv...  \n",
      "4  Analyze the different aspects such as offensiv...  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "print(training_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofLk7bt7_6dW",
    "outputId": "9d435fff-40f4-4f77-d890-b4ecb7e6cdff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['hatespeech', 'csType', 'counterspeech', 'Suggest', 'Relevance',\n",
      "       'Aggressive', 'Complexity', 'Comments', 'source', 'claim',\n",
      "       'centralTopic', 'speakerIntent', 'targetGroup', 'relevantPowerDynamics',\n",
      "       'hatespeechImplication', 'targetGroupEmotionalReaction',\n",
      "       'targetGroupCognitiveReaction', 'hatespeechOffensiveness', 'id',\n",
      "       'is_high_quality', 'hs_id', 'hatespeechTarget', 'powerDynamics',\n",
      "       'prompt_offensiveness', 'prompt_target_group', 'prompt_speaker_intent',\n",
      "       'prompt_power_dynamics', 'prompt_implication',\n",
      "       'prompt_emotional_reaction', 'prompt_cognitive_reaction',\n",
      "       'prompt_cs_generation'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "columns = training_data.columns\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "smSz_WAB_6dX"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class DialoGPTDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/hateBERT\")\n",
    "        self.bart_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "        # Intent label mapping\n",
    "        self.categories = {\n",
    "            'informative': 0,\n",
    "            'questioning': 1,\n",
    "            'denouncing': 2,\n",
    "            'positive': 3,\n",
    "            'humor': 4\n",
    "        }\n",
    "\n",
    "        # ✅ Create a mapping from hate speech → list of intent labels\n",
    "        self.intent_map = (\n",
    "            data.groupby(\"hatespeech\")[\"csType\"]\n",
    "            .apply(lambda x: [self.categories[t.lower()] for t in x.unique()])\n",
    "            .to_dict()\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # Tokenize hate speech\n",
    "        hate_inputs = self.tokenizer(\n",
    "            row[\"hatespeech\"],\n",
    "            return_tensors='pt',\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        # Tokenize counterspeech\n",
    "        counter_inputs = self.bart_tokenizer(\n",
    "            row[\"counterspeech\"],\n",
    "            return_tensors='pt',\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        intent_id = torch.tensor(self.categories[row[\"csType\"].lower()], dtype=torch.long)\n",
    "        all_intents = self.intent_map[row[\"hatespeech\"]]  # ✅ Look up all intents for this hate speech\n",
    "\n",
    "        return {\n",
    "            'input_ids': hate_inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': hate_inputs['attention_mask'].squeeze(0),\n",
    "            'counter_speech': counter_inputs['input_ids'].squeeze(0),\n",
    "            'intent_id': intent_id,\n",
    "            'raw_text': row[\"hatespeech\"],\n",
    "            'all_intents': all_intents  \n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    counter_speech = torch.stack([item['counter_speech'] for item in batch])\n",
    "    intent_id = torch.stack([item['intent_id'] for item in batch])\n",
    "    all_intents = [item['all_intents'] for item in batch]\n",
    "    raw_inputs = [item['raw_text'] for item in batch]\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'counter_speech': counter_speech,\n",
    "        'intent_id': intent_id,\n",
    "        'all_intents': all_intents,\n",
    "        'raw_inputs':raw_inputs\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjDAavD0_6dX",
    "outputId": "4fa0dee1-8852-4c22-dc9d-69e7432beafc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9532\n",
      "2971\n",
      "1470\n",
      "{'input_ids': tensor([  101,  2672,  1996,  4895,  2071,  2831,  2000,  2216,  4004,  1998,\n",
      "         3060,  3741,  3625,  2005,  3938,  1009,  1997,  1996, 10796,  1999,\n",
      "         1996, 17401,  2612,  1997, 22604,  2006,  2023, 14636,  2055,  4785,\n",
      "         2689,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'counter_speech': tensor([    0, 27847,   282,    75,   961,    15,     5,  5518,    33,    10,\n",
      "         2640,  1567,     5,  2147,   116,     2,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1]), 'intent_id': tensor(1), 'raw_text': 'Maybe the UN could talk to those asian and african nations responsible for 90+ of the pollution in the oceans instead of insisting on this bullshit about climate change.', 'all_intents': [0, 1, 2, 3]}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DialoGPTDataset(training_data)\n",
    "test_dataset = DialoGPTDataset(testing_data)\n",
    "validation_dataset = DialoGPTDataset(validation_data)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(validation_dataset))\n",
    "\n",
    "print(train_dataset[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LpupYCINMIin"
   },
   "outputs": [],
   "source": [
    "class FeatureEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, next_input):\n",
    "        super(FeatureEncoder, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained('GroNLP/hateBERT')\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = next_input\n",
    "\n",
    "        self.informative_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim, self.output_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.questioning_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim, self.output_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.denouncing_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim, self.output_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.positive_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim, self.output_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.humor_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim, self.output_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hate_speech_h = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        informative_e = self.informative_head(hate_speech_h)\n",
    "        questioning_e = self.questioning_head(hate_speech_h)\n",
    "        denouncing_e = self.denouncing_head(hate_speech_h)\n",
    "        positive_e = self.positive_head(hate_speech_h)\n",
    "        humor_e = self.humor_head(hate_speech_h)\n",
    "\n",
    "        return informative_e, questioning_e, denouncing_e, positive_e, humor_e, hate_speech_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RyXBzACbO4b6",
    "outputId": "5652bd54-a540-4980-bde6-8b0ddd5e8599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.0000, 0.0602, 0.0000, 0.2823, 0.4089, 0.0245, 0.1707, 0.0402, 0.2651,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6941, 0.3184, 0.2305, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0641, 0.0000, 0.0000, 0.0000,\n",
      "         0.1692, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2387, 0.0000,\n",
      "         0.0000, 0.0000, 0.1257, 0.1650, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0634, 0.1901, 0.0520, 0.0000, 0.0000, 0.0000, 0.0944, 0.0000, 0.1481,\n",
      "         0.4632, 0.0000, 0.1444, 0.0000, 0.0000, 0.1218, 0.0571, 0.0000, 0.0000,\n",
      "         0.2463, 0.0000, 0.1078, 0.0000, 0.0000, 0.0000, 0.0769, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.3521, 0.0000, 0.2064, 0.0000, 0.4716, 0.0000, 0.0000,\n",
      "         0.5140, 0.0000, 0.0000, 0.8616, 0.0698, 0.0000, 0.7839, 0.0000, 0.7415,\n",
      "         0.2588, 0.0000, 0.0000, 0.0552, 0.0000, 0.0000, 0.0174, 0.0000, 0.0000,\n",
      "         0.0000]], grad_fn=<ReluBackward0>), tensor([[0.0915, 0.0609, 0.0000, 0.0912, 0.0000, 0.0000, 0.3167, 0.0000, 0.4542,\n",
      "         0.2967, 0.0000, 0.0000, 0.0000, 0.4996, 0.2247, 0.3012, 0.0000, 0.2982,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0386, 0.0752, 0.2702, 0.2821, 0.3906,\n",
      "         0.0000, 0.4487, 0.0000, 0.0562, 0.0000, 0.0000, 0.0000, 0.4095, 0.1912,\n",
      "         0.0000, 0.0000, 0.0085, 0.0000, 0.0979, 0.5440, 0.2416, 0.0000, 0.0000,\n",
      "         0.1905, 0.0000, 0.0000, 0.4501, 0.0000, 0.0000, 0.3575, 0.0462, 0.0000,\n",
      "         0.3412, 0.0000, 0.1150, 0.0000, 0.0000, 0.0210, 0.1658, 0.6685, 0.0000,\n",
      "         0.0000, 0.1131, 0.0000, 0.0000, 0.0000, 0.0192, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0297, 0.0000, 0.0000, 0.0000, 0.0000, 0.0913,\n",
      "         0.0777, 0.0686, 0.0000, 0.0000, 0.1320, 0.3471, 0.0000, 0.3384, 0.1536,\n",
      "         0.0000, 0.1412, 0.0000, 0.0000, 0.0000, 0.0000, 0.2665, 0.0000, 0.5213,\n",
      "         0.2350]], grad_fn=<ReluBackward0>), tensor([[0.0000, 0.1245, 0.0000, 0.5796, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1841, 0.0000, 0.0000, 0.3295, 0.1108, 0.3081, 0.0000, 0.0000, 0.2659,\n",
      "         0.0000, 0.0000, 0.3238, 0.4170, 0.0285, 0.0481, 0.0884, 0.2499, 0.0000,\n",
      "         0.0000, 0.0794, 0.1666, 0.2112, 0.0000, 0.0000, 0.0000, 0.5944, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.2204, 0.5314, 0.1881, 0.0461, 0.0000,\n",
      "         0.4555, 0.0000, 0.2908, 0.2361, 0.0797, 0.2771, 0.3501, 0.3606, 0.0150,\n",
      "         0.0720, 0.0000, 0.3984, 0.0683, 0.0000, 0.0000, 0.7351, 0.0457, 0.2566,\n",
      "         0.0000, 0.1181, 0.3187, 0.0690, 0.0000, 0.1229, 0.0000, 0.1016, 0.0000,\n",
      "         0.0000, 0.0382, 0.1400, 0.0000, 0.0000, 0.0454, 0.0000, 0.0000, 0.0345,\n",
      "         0.5512, 0.0000, 0.0215, 0.1997, 0.0000, 0.0000, 0.0000, 0.0000, 0.0399,\n",
      "         0.0393, 0.0000, 0.3977, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]], grad_fn=<ReluBackward0>), tensor([[0.0000, 0.0000, 0.0321, 0.1893, 0.0000, 0.1542, 0.0000, 0.2452, 0.0000,\n",
      "         0.0000, 0.5953, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1404, 0.5227, 0.1323, 0.0870, 0.2000, 0.0000, 0.0000, 0.0000,\n",
      "         0.5882, 0.6366, 0.1342, 0.0000, 0.2902, 0.0000, 0.0368, 0.0000, 0.2959,\n",
      "         0.0000, 0.2721, 0.1335, 0.3733, 0.2290, 0.2631, 0.0000, 0.2772, 0.0000,\n",
      "         0.0000, 0.2013, 0.0000, 0.0000, 0.0000, 0.2018, 0.3299, 0.2591, 0.0000,\n",
      "         0.1309, 0.0000, 0.0000, 0.0000, 0.0000, 0.2942, 0.2633, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1273, 0.0000, 0.1846, 0.0478, 0.0000,\n",
      "         0.4215, 0.1608, 0.0000, 0.3991, 0.0000, 0.0000, 0.0000, 0.0000, 0.2630,\n",
      "         0.0000, 0.0000, 0.0000, 0.2639, 0.0000, 0.0000, 0.6079, 0.0000, 0.1681,\n",
      "         0.0000, 0.0410, 0.3510, 0.3407, 0.5143, 0.1390, 0.0583, 0.0000, 0.1803,\n",
      "         0.0000]], grad_fn=<ReluBackward0>), tensor([[0.0000, 0.0000, 0.2075, 0.1055, 0.0000, 0.1559, 0.3517, 0.4597, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0086, 0.0000, 0.0000, 0.1845, 0.0000, 0.0000,\n",
      "         0.5227, 0.1735, 0.0337, 0.0062, 0.0404, 0.4193, 0.0930, 0.0000, 0.3101,\n",
      "         0.3495, 0.0969, 0.0000, 0.1583, 0.2235, 0.3871, 0.1229, 0.0588, 0.0000,\n",
      "         0.0000, 0.0000, 0.0227, 0.0845, 0.0000, 0.2562, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2075, 0.0000, 0.0731, 0.0383,\n",
      "         0.6329, 0.2474, 0.0000, 0.3882, 0.1230, 0.0000, 0.0000, 0.0625, 0.0000,\n",
      "         0.4353, 0.0000, 0.2106, 0.0000, 0.0000, 0.0000, 0.1971, 0.0838, 0.0000,\n",
      "         0.1832, 0.0000, 0.1921, 0.2810, 0.4655, 0.0798, 0.0000, 0.0000, 0.0292,\n",
      "         0.0000, 0.0000, 0.0000, 0.4532, 0.0000, 0.0000, 0.0000, 0.3750, 0.3753,\n",
      "         0.2770, 0.0000, 0.0000, 0.1329, 0.6137, 0.0000, 0.2321, 0.1734, 0.1002,\n",
      "         0.6091]], grad_fn=<ReluBackward0>), tensor([[-1.2317e-01,  2.5760e-01, -6.8412e-01,  4.6821e-01,  8.0943e-01,\n",
      "         -2.5792e-01,  1.5351e-01,  1.3639e+00, -1.0266e+00,  1.3265e-01,\n",
      "         -6.1926e-01,  2.1133e-01, -4.9460e-01,  7.6190e-01, -6.9545e-02,\n",
      "          6.8312e-01,  3.7021e-01, -4.6095e-01,  3.3195e-01,  5.0576e-01,\n",
      "          2.4192e-01, -2.2491e-01,  3.1872e-01,  8.5852e-01,  1.3688e-01,\n",
      "         -4.0543e-01,  5.2988e-01,  7.3655e-01, -1.8011e-01,  4.7114e-01,\n",
      "          1.1463e-01, -1.9435e-01,  3.9097e-02, -1.4291e-01, -1.6129e-01,\n",
      "         -4.2231e-01, -7.1753e-01,  2.2965e-01, -1.0829e-01,  4.8140e-01,\n",
      "         -2.8108e-01,  3.4257e-02, -2.7372e-01, -4.2401e-01,  1.7613e-01,\n",
      "          5.1272e-01, -1.1607e+00,  9.1711e-01,  4.1866e-01,  1.1515e-01,\n",
      "          2.4045e-02,  4.7473e-01,  5.9433e-01, -1.7153e-01,  1.6978e-01,\n",
      "          6.4695e-01, -4.9544e-01,  4.1428e-02, -1.0952e+00,  3.7566e-03,\n",
      "          2.9936e-01, -5.2084e-01, -4.3595e-02,  3.3521e-01,  1.2925e-01,\n",
      "          2.1945e-01,  1.5714e-01, -5.9910e-01, -3.5057e-02,  4.0079e-01,\n",
      "         -9.8173e-02,  3.8298e-01, -7.9815e-01, -6.8081e-02, -5.0185e-01,\n",
      "         -1.1499e+00,  4.8555e-01,  5.9444e-01, -1.9008e-01,  2.0195e-01,\n",
      "         -2.1091e-01, -7.6863e-02, -9.0526e-01,  4.3959e-01, -4.6773e-01,\n",
      "         -1.8348e-01,  6.2463e-02,  1.1052e+00, -3.8372e-01,  1.5440e-01,\n",
      "         -2.4588e-01,  3.0191e-01,  3.5213e-01, -1.3883e-01,  1.1457e+00,\n",
      "         -4.6540e-01,  3.4221e-01,  2.6335e-01,  5.6252e-01,  4.1568e-01,\n",
      "         -7.6780e-01, -1.1107e+00,  4.1170e-01,  4.5548e-01, -5.5886e-01,\n",
      "         -6.0578e-01, -6.8058e-01, -3.3178e-01, -2.8615e-01, -1.0600e+00,\n",
      "          1.1757e+00, -1.6308e-01,  3.4663e-01, -6.9612e-01,  1.6039e-01,\n",
      "          6.7695e-01,  6.8573e-01, -4.1015e-01, -1.3529e-01,  3.5348e-01,\n",
      "          3.2437e-01,  2.6495e-01, -3.6576e-03, -7.7232e-02, -4.1760e-01,\n",
      "          1.2196e-01,  4.7131e-01, -1.5215e-01, -3.7126e-01, -1.7947e-01,\n",
      "          4.6674e-01,  7.4275e-01,  4.2055e-01, -3.5353e-01,  4.1961e-01,\n",
      "         -1.3581e-01,  1.5983e-01, -4.0132e-01,  1.1101e+00, -1.9269e-01,\n",
      "          6.2207e-01, -2.3745e-01, -1.8398e+00,  2.3780e-01,  9.0787e-02,\n",
      "          6.5546e-01, -3.2986e-01, -1.7356e-01, -1.8928e-01,  7.1867e-01,\n",
      "         -1.2658e-01,  6.1733e-02,  1.8678e-01, -3.1777e-01, -5.4623e-01,\n",
      "         -2.5240e-01, -4.4977e-01, -3.9046e-02,  7.4262e-01, -5.8837e-02,\n",
      "          1.6054e-01, -5.0865e-01,  1.7492e-01, -2.7634e-01,  1.2887e-01,\n",
      "          8.1764e-02,  5.0241e-02,  9.8645e-01, -4.3364e-01,  1.4911e-01,\n",
      "         -5.8697e-01, -6.5167e-01,  9.5687e-01, -1.8770e-01,  4.1894e-01,\n",
      "         -3.7152e-01,  6.5440e-01,  4.5212e-01, -8.6196e-02, -2.2374e-01,\n",
      "         -8.7534e-01,  2.2465e-01, -2.1325e-01,  6.3038e-01,  1.1065e-01,\n",
      "         -5.4311e-01, -5.1110e-02,  5.6019e-01,  1.1177e+00, -3.2925e-01,\n",
      "         -5.6906e-01,  1.7327e-01,  6.8197e-02,  3.8716e-01, -5.0439e-02,\n",
      "         -2.0217e-01,  1.4270e-01,  7.4871e-01,  2.7728e-01,  1.7624e-01,\n",
      "         -9.2496e-01,  2.3841e-01,  6.4823e-01, -4.1757e-01, -6.7736e-01,\n",
      "          2.4559e+00, -1.8675e-01,  1.0047e-01,  1.5118e-01,  5.0533e-01,\n",
      "         -4.2134e-01, -8.7620e-04, -4.5003e-02,  1.6084e-01, -6.0728e-01,\n",
      "          1.9793e-01, -7.8367e-01,  2.2306e-01, -2.2293e-01,  2.4963e-01,\n",
      "          6.9048e-01,  6.4160e-01, -1.2937e-01,  7.0448e-02, -5.7323e-02,\n",
      "          4.0691e-01,  5.9287e-01, -5.5968e-02,  8.0828e-01, -2.3386e-01,\n",
      "         -5.2182e-01, -3.5160e-01,  3.1023e-02, -1.1959e-01,  2.9239e-02,\n",
      "          1.0504e-01, -1.4949e-01,  1.1976e-01,  8.0820e-01,  8.3419e-02,\n",
      "         -3.6125e-01, -2.9663e-01, -6.0953e-01,  5.0977e-01,  4.5629e-01,\n",
      "          4.5553e-01,  5.1052e-01,  2.2577e-01, -3.3124e-01,  4.3715e-01,\n",
      "         -3.4963e-01,  3.9342e-01, -1.8830e-03, -3.4032e-01,  1.5296e-01,\n",
      "          8.7104e-02, -1.6105e-01,  5.3819e-01,  2.1507e-01, -6.3888e-01,\n",
      "         -7.4827e-02,  5.1826e-03, -1.8684e-01,  5.4644e-02, -2.3305e-01,\n",
      "         -7.6733e-01,  7.9099e-01,  9.4913e-02,  6.2855e-01,  1.3569e-01,\n",
      "          3.5545e-01,  6.2363e-01,  4.5773e-02, -1.6890e+00, -2.3884e-02,\n",
      "         -5.0119e-01,  1.3611e-01,  4.0873e-01, -1.5019e-01,  5.2313e-01,\n",
      "          6.4430e-01,  1.1717e+00, -9.5832e-01, -6.1409e-01, -8.0561e-01,\n",
      "         -1.6220e-01,  2.4920e-01, -8.2227e-01, -4.3588e-02, -2.6612e-02,\n",
      "          3.4783e-01, -7.0667e-01, -2.0241e-01, -9.8403e-01, -2.2646e-02,\n",
      "          2.2685e-01,  1.3894e-01, -6.7778e-01,  6.9484e-01, -5.4873e-02,\n",
      "         -6.6487e-02, -5.5729e-01,  3.6755e-02,  2.2070e-02, -7.1231e-01,\n",
      "          2.7916e-01, -4.7228e-04, -8.7007e-01, -3.1925e+00, -3.8667e-01,\n",
      "          1.9817e-01,  6.7628e-01, -1.0927e-01,  1.7100e-01,  2.4088e-01,\n",
      "          7.8297e-02, -4.5646e-01,  1.0410e-01,  3.8411e-01,  5.1960e-01,\n",
      "          9.7735e-02, -5.6284e-01,  4.5912e-01,  2.0076e-01,  3.5869e-01,\n",
      "         -2.8795e-03,  3.4292e-01,  4.7845e-01, -5.0375e-02, -1.2040e-01,\n",
      "         -2.4911e-01,  2.2337e-01,  4.4491e-01,  2.2470e-01,  2.1969e-01,\n",
      "         -7.0901e-01, -7.5797e-02, -4.0109e-01,  2.1902e-01, -3.0490e-01,\n",
      "         -1.5279e-01, -1.9591e-01,  3.9023e-01,  3.0864e-01, -9.3815e-02,\n",
      "          5.2041e-01, -2.5760e-01,  3.0921e-01, -2.5657e-01, -6.5282e-01,\n",
      "         -4.6588e-02, -3.0058e-01,  9.0686e-01, -1.7764e-01, -1.2411e-01,\n",
      "         -8.6077e-01,  2.9516e-01,  4.3452e-01,  2.8654e-01,  5.3828e-01,\n",
      "          8.1635e-01, -3.2586e-01, -2.4166e-01, -2.7436e-01,  7.0768e-01,\n",
      "          1.5937e-01, -2.4538e-01,  1.2337e-01,  1.3415e-01,  4.9002e-01,\n",
      "          3.2754e-01,  2.2374e-02, -1.3236e-01, -5.5545e-02, -2.2924e-01,\n",
      "         -5.6747e-01,  1.2857e-01, -2.7168e-01, -9.6040e-01,  3.1013e-01,\n",
      "         -5.3072e-01, -1.0119e+00, -2.3787e-01, -1.0075e+00, -4.9160e-01,\n",
      "         -3.0076e-01, -1.0902e-01,  2.2922e-02, -6.6215e-01, -2.1729e-01,\n",
      "         -1.3537e-01,  8.3462e-01,  5.5448e-01, -1.3850e-01, -4.5189e-01,\n",
      "         -3.4199e-01,  5.7501e-02, -3.8554e-02,  4.5402e-01, -9.7354e-01,\n",
      "         -2.1974e-01,  5.9555e-02,  8.3421e-01, -6.1270e-02,  2.3983e-01,\n",
      "         -6.1180e-01,  2.1610e-01, -5.2339e-01, -4.2720e-01, -6.3771e-01,\n",
      "         -6.5095e-01, -7.1220e-01,  3.3096e-01, -2.3564e-01, -1.2212e-01,\n",
      "          9.8597e-01, -2.9605e-02,  1.9035e-01,  4.4752e-01, -7.2772e-01,\n",
      "          1.5666e-01, -2.6520e-01,  7.5874e-01,  2.2994e-01, -8.1482e-02,\n",
      "          4.4986e-01,  2.9370e-02,  3.9530e-01,  1.3770e-01,  9.5294e-01,\n",
      "         -5.7926e-02, -2.3266e-01, -3.7861e-02, -4.3921e-01, -2.4357e-01,\n",
      "         -4.3165e-01, -7.9695e-02, -6.7831e-01, -2.0219e-01,  4.1504e-02,\n",
      "          1.5968e-01, -4.6000e-02,  6.7886e-01, -3.3935e-01, -1.3622e-01,\n",
      "         -7.7347e-01, -2.9268e-02,  4.9231e-01, -6.4105e-01,  8.3318e-02,\n",
      "          4.7684e-01, -4.7184e-01, -3.9823e-01,  6.0697e-01, -5.7826e-01,\n",
      "          2.1868e-01,  3.8097e-01, -2.8612e-03, -4.8541e-01, -1.0757e+00,\n",
      "         -7.1333e-01,  4.5390e-01, -2.0179e-01,  4.3082e-01,  9.3196e-01,\n",
      "          2.4342e-01, -8.4648e-02, -1.6379e-01, -2.6147e-01,  1.8407e-01,\n",
      "         -8.2096e-01,  6.0947e-02,  5.6156e-01,  8.4814e-01, -5.1333e-01,\n",
      "         -4.9439e-01,  2.7934e-01,  4.8919e-02,  1.4163e-01, -2.6337e-01,\n",
      "         -3.5863e-01, -2.6333e-01, -4.0775e-01, -1.9226e-01, -1.3126e-01,\n",
      "         -4.9541e-02, -2.3981e-01,  8.2045e-01, -3.0229e-03, -4.5013e-01,\n",
      "         -1.8484e-01,  2.3027e-01,  3.3701e-01, -1.0127e-01, -3.4452e-01,\n",
      "         -1.9135e-01, -2.0341e-01,  3.9551e-01,  3.9827e-01,  5.0469e-01,\n",
      "         -1.2788e-01,  6.4177e-01, -6.9860e-01, -2.0280e-01,  1.3127e-01,\n",
      "          2.2284e-01, -3.1996e-01,  2.2150e-01,  1.7712e-01,  2.1998e-01,\n",
      "         -7.3683e-01,  5.1713e-01,  3.6159e-02, -3.6509e-01, -4.3328e-01,\n",
      "         -1.2380e-01,  1.8382e-01,  4.6504e-01, -9.2250e-01, -2.0718e-01,\n",
      "         -2.3360e-01,  2.2939e-01,  6.6333e-01, -6.3954e-01,  5.7905e-02,\n",
      "         -3.3022e-01,  2.4985e-02, -2.6652e-01, -1.8399e-01,  4.3828e-01,\n",
      "          4.8261e-01, -4.4609e-01,  8.0297e-01,  1.0476e-01, -1.1147e+00,\n",
      "         -3.6112e-06, -3.2108e-01, -2.2974e-01, -5.5197e-01, -9.1232e-01,\n",
      "          1.5148e-01,  6.9941e-01, -5.7468e-02, -2.8318e-01, -3.0531e-01,\n",
      "          7.6689e-02, -2.6689e-01, -4.4704e-01,  4.7444e-01,  4.6342e-02,\n",
      "         -3.6947e-01, -7.6067e-01, -7.5091e-01,  5.0563e-01, -6.5453e-01,\n",
      "          4.0746e-01,  2.8565e-01, -2.2232e-01, -6.4757e-01, -5.3656e-01,\n",
      "         -1.3978e-03, -1.0704e+00,  5.3240e-01, -2.9782e-01, -5.0650e-01,\n",
      "         -4.1498e-02, -4.4396e-01,  2.0083e-02,  1.7154e-01, -3.1965e-02,\n",
      "         -6.9168e-01, -4.4352e-01,  8.8167e-01, -2.6560e-01, -5.4387e-01,\n",
      "         -3.6767e-01,  3.1593e-01, -1.7683e-01,  2.3699e-01, -9.4361e-02,\n",
      "         -1.5392e-01, -4.3483e-02,  4.9739e-01,  8.1368e-02, -6.3719e-03,\n",
      "         -2.6060e-01,  1.0677e+00, -1.2285e+00,  1.0485e+00,  1.0843e+00,\n",
      "         -2.1918e-01, -1.6651e-01, -8.3577e-03, -1.3463e-01, -1.6047e-01,\n",
      "          9.8834e-02,  6.2214e-01, -2.0275e-01,  2.0175e-01, -3.1997e-01,\n",
      "         -4.9679e-01,  3.3211e-01,  3.8114e-01,  5.7119e-01,  5.1291e-01,\n",
      "          1.8488e-02, -4.2402e-01,  1.7790e-01, -1.1871e+00, -2.0845e-01,\n",
      "         -3.9290e-02,  5.5680e-01,  8.9255e-01, -5.8632e-02,  3.3880e-01,\n",
      "          9.5283e-02,  9.5194e-03, -2.3480e-02, -1.6059e-01,  3.4324e-01,\n",
      "          3.7971e-02,  2.7206e-01,  1.3271e-01,  3.6109e-01,  2.9792e-01,\n",
      "          5.6152e-01,  7.6749e-01, -4.0285e-02, -1.6407e-01, -6.9849e-01,\n",
      "         -5.7901e-01,  7.9291e-01,  9.6178e-02,  8.0835e-03, -6.0179e-01,\n",
      "         -2.1675e-01,  1.0679e-01,  9.2902e-01,  3.8888e-01,  1.2707e+00,\n",
      "          2.8934e-01, -7.8272e-01, -5.9534e-01,  1.0079e+00,  1.8520e-01,\n",
      "         -5.8274e-01, -7.6067e-01,  9.1246e-01,  7.4106e-01,  3.6579e-01,\n",
      "          2.6696e-01,  3.0449e-01,  4.0488e-01, -4.7377e-02, -4.9712e-01,\n",
      "          3.2128e-01,  7.3942e-02, -2.2706e-01, -4.1623e-02, -4.0169e-01,\n",
      "         -6.9343e-02, -6.2744e-02,  4.8091e-01,  2.4996e-01,  2.6361e-01,\n",
      "          1.2227e-01,  8.5500e-01, -1.2661e+00, -1.7783e-01, -5.6633e-02,\n",
      "          8.3859e-01,  1.1827e-01,  4.8322e-01,  4.5807e-01, -1.1281e-02,\n",
      "         -5.8511e-02,  3.8427e-01, -5.6437e-03,  1.6789e-01,  1.0114e-01,\n",
      "          1.1220e-01, -5.1211e-01,  8.3886e-01, -2.4258e-01, -8.4999e-01,\n",
      "         -4.9027e-01,  3.7390e-01,  4.1485e-01, -3.3120e-01,  9.7744e-01,\n",
      "         -4.5607e-01, -7.3515e-03, -7.3092e-02,  8.7236e-02, -3.0596e-01,\n",
      "          4.6145e-01,  1.8278e-01,  1.9414e-01,  5.5773e-01, -6.4275e-01,\n",
      "         -4.8545e-01,  5.0916e-01, -5.0577e-01, -9.8773e-01,  3.9029e-01,\n",
      "         -7.1127e-02,  1.7022e-01, -6.7253e-01,  7.6379e-01,  1.0064e-01,\n",
      "         -1.7974e-01,  2.8179e-01, -2.8583e-02, -5.7483e-01, -2.1858e-01,\n",
      "          3.5220e-01,  2.2009e-02,  1.3801e-01,  1.1380e+00,  3.3350e-02,\n",
      "         -1.3364e-01,  3.1937e-01, -6.0456e-01,  4.3839e-01,  1.6217e-02,\n",
      "          2.9566e-01,  4.8918e-01, -3.1255e-01,  3.8324e-01, -7.4790e-01,\n",
      "         -6.5821e-01,  5.5319e-01,  1.8778e-01, -4.6856e-01, -8.2878e-02,\n",
      "         -1.4972e-02, -5.5359e-01,  7.8357e-01, -6.4829e-01,  2.1436e-03,\n",
      "          2.2975e-01,  3.1753e-02, -5.4749e-01, -1.0020e+00, -8.0302e-02,\n",
      "          3.9623e-02, -5.9870e-02, -3.5461e-01, -3.1486e-02,  1.2653e-01,\n",
      "         -2.3913e-01,  3.7739e-01, -3.7753e+00, -9.8182e-01, -2.3795e-01,\n",
      "         -8.2952e-01, -5.5751e-01,  2.5009e-01,  7.1639e-01, -4.8086e-01,\n",
      "         -1.4179e-02, -1.4914e-01, -1.1136e-01, -2.0262e-01,  1.9009e-02,\n",
      "         -7.5536e-01,  5.8692e-01,  2.5884e-01]], grad_fn=<SliceBackward0>))\n"
     ]
    }
   ],
   "source": [
    "feature_encoder = FeatureEncoder(input_dim=128, hidden_dim=768, next_input=100)\n",
    "print(feature_encoder(train_dataset[0]['input_ids'].unsqueeze(0), train_dataset[0]['attention_mask'].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8qxFuKQrRtuO"
   },
   "outputs": [],
   "source": [
    "class CounterSpeechNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, encoder_output, max_length):\n",
    "        super(CounterSpeechNetwork, self).__init__()\n",
    "\n",
    "        self.feature_encoder = FeatureEncoder(input_dim, hidden_dim, encoder_output)\n",
    "\n",
    "        self.informative_decoder = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "        self.questioning_decoder = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "        self.denouncing_decoder = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "        self.positive_decoder = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "        self.humor_decoder = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "\n",
    "        self.informative_fusion = torch.nn.Linear(hidden_dim + encoder_output, self.informative_decoder.config.d_model)\n",
    "        self.questioning_fusion = torch.nn.Linear(hidden_dim + encoder_output, self.questioning_decoder.config.d_model)\n",
    "        self.denouncing_fusion = torch.nn.Linear(hidden_dim + encoder_output, self.denouncing_decoder.config.d_model)\n",
    "        self.positive_fusion = torch.nn.Linear(hidden_dim + encoder_output, self.positive_decoder.config.d_model)\n",
    "        self.humor_fusion = torch.nn.Linear(hidden_dim + encoder_output, self.humor_decoder.config.d_model)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, intent_id, counter_speech=None):\n",
    "        informative_e, questioning_e, denouncing_e, positive_e, humor_e, hate_speech_h = self.feature_encoder(input_ids, attention_mask)\n",
    "\n",
    "        batch_size = input_ids.size(0)\n",
    "\n",
    "        fused = torch.zeros(batch_size, 1, self.informative_decoder.config.d_model, device=input_ids.device)\n",
    "    \n",
    "        for i in range(batch_size):\n",
    "            if intent_id[i] == 0:\n",
    "                fused[i] = self.informative_fusion(torch.cat((hate_speech_h[i], informative_e[i]), dim=-1)).unsqueeze(0)\n",
    "            elif intent_id[i] == 1:\n",
    "                fused[i] = self.questioning_fusion(torch.cat((hate_speech_h[i], questioning_e[i]), dim=-1)).unsqueeze(0)\n",
    "            elif intent_id[i] == 2:\n",
    "                fused[i] = self.denouncing_fusion(torch.cat((hate_speech_h[i], denouncing_e[i]), dim=-1)).unsqueeze(0)\n",
    "            elif intent_id[i] == 3:\n",
    "                fused[i] = self.positive_fusion(torch.cat((hate_speech_h[i], positive_e[i]), dim=-1)).unsqueeze(0)\n",
    "            elif intent_id[i] == 4:\n",
    "                fused[i] = self.humor_fusion(torch.cat((hate_speech_h[i], humor_e[i]), dim=-1)).unsqueeze(0)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid intent_id: {intent_id[i]}\")\n",
    "\n",
    "        if counter_speech is not None:\n",
    "            losses = []\n",
    "            for i in range(batch_size):\n",
    "                if intent_id[i] == 0:\n",
    "                    output = self.informative_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0))\n",
    "                elif intent_id[i] == 1:\n",
    "                    output = self.questioning_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0))\n",
    "                elif intent_id[i] == 2:\n",
    "                    output = self.denouncing_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0))\n",
    "                elif intent_id[i] == 3:\n",
    "                    output = self.positive_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0))\n",
    "                elif intent_id[i] == 4:\n",
    "                    output = self.humor_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0))\n",
    "                losses.append(output.loss)\n",
    "            avg_loss = sum(losses) / len(losses)  # Average loss across the batch\n",
    "            return None, avg_loss  # No decoded text during training\n",
    "        else:\n",
    "            decoded_texts = []\n",
    "            for i in range(batch_size):\n",
    "                if intent_id[i] == 0:\n",
    "                    output = self.informative_decoder.generate(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), max_length=self.max_length, num_beams=4, early_stopping=True)\n",
    "                elif intent_id[i] == 1:\n",
    "                    output = self.questioning_decoder.generate(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), max_length=self.max_length, num_beams=4, early_stopping=True)\n",
    "                elif intent_id[i] == 2:\n",
    "                    output = self.denouncing_decoder.generate(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), max_length=self.max_length, num_beams=4, early_stopping=True)\n",
    "                elif intent_id[i] == 3:\n",
    "                    output = self.positive_decoder.generate(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), max_length=self.max_length, num_beams=4, early_stopping=True)\n",
    "                elif intent_id[i] == 4:\n",
    "                    output = self.humor_decoder.generate(encoder_outputs=BaseModelOutput(last_hidden_state=fused[i].unsqueeze(0)), max_length=self.max_length, num_beams=4, early_stopping=True)\n",
    "                decoded_texts.append(self.tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "            return decoded_texts, None  # Decoded text during inference, no loss\n",
    "    def judge_responses(self,input_ids, attention_mask,counter_speech=None):\n",
    "        \n",
    "        informative_e, questioning_e, denouncing_e, positive_e, humor_e, hate_speech_h = self.feature_encoder(input_ids, attention_mask)\n",
    "\n",
    "        batch_size = input_ids.size(0)\n",
    "        output_dict = {'informative':[],'questioning':[],'denouncing':[],'positive':[],'humor':[]}\n",
    "        informative_fused = torch.zeros(batch_size, 1, self.informative_decoder.config.d_model, device=input_ids.device)\n",
    "        questioning_fused = torch.zeros(batch_size, 1, self.informative_decoder.config.d_model, device=input_ids.device)\n",
    "        denouncing_fused = torch.zeros(batch_size, 1, self.informative_decoder.config.d_model, device=input_ids.device)\n",
    "        positive_fused = torch.zeros(batch_size, 1, self.informative_decoder.config.d_model, device=input_ids.device)\n",
    "        humor_fused = torch.zeros(batch_size, 1, self.informative_decoder.config.d_model, device=input_ids.device)\n",
    "        \n",
    "    \n",
    "        for i in range(batch_size):\n",
    "            informative_fused[i] = self.informative_fusion(torch.cat((hate_speech_h[i], informative_e[i]), dim=-1)).unsqueeze(0)\n",
    "            questioning_fused[i] = self.questioning_fusion(torch.cat((hate_speech_h[i], questioning_e[i]), dim=-1)).unsqueeze(0)\n",
    "            denouncing_fused[i] = self.denouncing_fusion(torch.cat((hate_speech_h[i], denouncing_e[i]), dim=-1)).unsqueeze(0)\n",
    "            positive_fused[i] = self.positive_fusion(torch.cat((hate_speech_h[i], positive_e[i]), dim=-1)).unsqueeze(0)\n",
    "            humor_fused[i] = self.humor_fusion(torch.cat((hate_speech_h[i], humor_e[i]), dim=-1)).unsqueeze(0)\n",
    "\n",
    "\n",
    "            output_dict['informative'].append(self.informative_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=informative_fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0)))\n",
    "            output_dict['questioning'].append(self.questioning_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=questioning_fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0)))\n",
    "            output_dict['denouncing'].append(self.denouncing_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=denouncing_fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0)))\n",
    "            output_dict['positive'].append(self.positive_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=positive_fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0)))\n",
    "            output_dict['humor'].append(self.humor_decoder(encoder_outputs=BaseModelOutput(last_hidden_state=humor_fused[i].unsqueeze(0)), labels=counter_speech[i].unsqueeze(0)))\n",
    "\n",
    "\n",
    "        return output_dict\n",
    "            \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dVxqr6WWc8j",
    "outputId": "1bf35674-29e9-4ad9-b9a1-65898292fc9e"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "counterspeechnetwork = CounterSpeechNetwork(input_dim=128, hidden_dim=768, encoder_output=256, max_length=50)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    intent_id = batch['intent_id']\n",
    "    counter_speech = batch['counter_speech']\n",
    "\n",
    "    print(counterspeechnetwork(input_ids, attention_mask, intent_id, counter_speech))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "du_G6NX9c_MN",
    "outputId": "01eb8d9f-36ee-4632-a7ff-bb39026481cd"
   },
   "outputs": [],
   "source": [
    "model = CounterSpeechNetwork(input_dim=128, hidden_dim=768, encoder_output=256, max_length=50)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True,collate_fn = custom_collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True,collate_fn = custom_collate_fn)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=32, shuffle=True,collate_fn = custom_collate_fn)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    # Wrap train_dataloader with tqdm for training progress\n",
    "    train_loop = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs} [Train]\", leave=False)\n",
    "    for batch in train_loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        counter_speech = batch['counter_speech'].to(device)\n",
    "        intent_ids = batch['intent_id'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Process the entire batch at once\n",
    "        _, loss = model(input_ids, attention_mask, intent_ids, counter_speech)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Update tqdm with current batch loss\n",
    "        train_loop.set_postfix({'batch_loss': loss.item(), 'avg_loss': total_train_loss / (train_loop.n + 1)})\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} | Train Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    # Wrap validation_dataloader with tqdm for validation progress\n",
    "    val_loop = tqdm(validation_dataloader, desc=f\"Epoch {epoch + 1}/{epochs} [Validation]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            counter_speech = batch['counter_speech'].to(device)\n",
    "            intent_ids = batch['intent_id'].to(device)\n",
    "\n",
    "            # Process the entire batch at once\n",
    "            _, loss = model(input_ids, attention_mask, intent_ids, counter_speech)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            # Update tqdm with current batch loss\n",
    "            val_loop.set_postfix({'batch_loss': loss.item(), 'avg_loss': total_val_loss / (val_loop.n + 1)})\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} | Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'architecture_parameters.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ChcK0JiiHeC"
   },
   "outputs": [],
   "source": [
    "# Test evaluation\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "test_references = []\n",
    "\n",
    "test_loop = tqdm(test_dataloader, desc=\"Test Evaluation\", leave=True)\n",
    "with torch.no_grad():\n",
    "    for batch in test_loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        counter_speech = batch['counter_speech'].to(device)  # Reference texts\n",
    "        intent_ids = batch['intent_id'].to(device)\n",
    "\n",
    "        predictions, _ = model(input_ids, attention_mask, intent_ids)\n",
    "\n",
    "        references = [model.tokenizer.decode(cs, skip_special_tokens=True) for cs in counter_speech]\n",
    "\n",
    "        test_predictions.extend(predictions)\n",
    "        test_references.extend(references)\n",
    "\n",
    "        test_loop.set_postfix({'predictions': len(test_predictions)})\n",
    "\n",
    "# Final output\n",
    "print(f\"Generated {len(test_predictions)} test predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R, F1 = bert_score(test_predictions, test_references, lang=\"en\", verbose=True)\n",
    "\n",
    "# Final output\n",
    "print(f\"Generated {len(test_predictions)} test predictions\")\n",
    "print(f\"BERTScore - Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "for pred, ref in zip(test_predictions, test_references):\n",
    "    scores = scorer.score(ref, pred)  # ROUGE scores for each prediction-reference pair\n",
    "    rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "    rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "    rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "# Compute average ROUGE scores\n",
    "avg_rouge1 = sum(rouge_scores['rouge1']) / len(rouge_scores['rouge1'])\n",
    "avg_rouge2 = sum(rouge_scores['rouge2']) / len(rouge_scores['rouge2'])\n",
    "avg_rougeL = sum(rouge_scores['rougeL']) / len(rouge_scores['rougeL'])\n",
    "\n",
    "print(f\"ROUGE - Rouge1: {avg_rouge1:.4f}, Rouge2: {avg_rouge2:.4f}, RougeL: {avg_rougeL:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred, ref in zip(test_predictions, test_references):\n",
    "    print(f'Generated: {pred}')\n",
    "    print(f'Actual: {ref}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1406414/785172572.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"architecture_parameters.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CounterSpeechNetwork(\n",
       "  (feature_encoder): FeatureEncoder(\n",
       "    (model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (informative_head): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (questioning_head): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (denouncing_head): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (positive_head): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (humor_head): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (informative_decoder): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartEncoderLayer(\n",
       "            (self_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartDecoderLayer(\n",
       "            (self_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  )\n",
       "  (questioning_decoder): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartEncoderLayer(\n",
       "            (self_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartDecoderLayer(\n",
       "            (self_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  )\n",
       "  (denouncing_decoder): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartEncoderLayer(\n",
       "            (self_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartDecoderLayer(\n",
       "            (self_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  )\n",
       "  (positive_decoder): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartEncoderLayer(\n",
       "            (self_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartDecoderLayer(\n",
       "            (self_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  )\n",
       "  (humor_decoder): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartEncoderLayer(\n",
       "            (self_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartDecoderLayer(\n",
       "            (self_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  )\n",
       "  (informative_fusion): Linear(in_features=1024, out_features=768, bias=True)\n",
       "  (questioning_fusion): Linear(in_features=1024, out_features=768, bias=True)\n",
       "  (denouncing_fusion): Linear(in_features=1024, out_features=768, bias=True)\n",
       "  (positive_fusion): Linear(in_features=1024, out_features=768, bias=True)\n",
       "  (humor_fusion): Linear(in_features=1024, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CounterSpeechNetwork(input_dim=128, hidden_dim=768, encoder_output=256, max_length=50)\n",
    "model.load_state_dict(torch.load(\"architecture_parameters.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_scoring(counterspeeches, device=\"cuda\"):\n",
    "\n",
    "    judge_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "    judge_model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\").to(device)\n",
    "    judge_model.eval()\n",
    "\n",
    "    selected_intents = {}\n",
    "\n",
    "    for hate_speech, intent_responses in counterspeeches.items():\n",
    "        best_intent = None\n",
    "        best_score = float(\"-inf\")\n",
    "\n",
    "        for intent, response in intent_responses.items():\n",
    "            # Construct input for DialoGPT: HateSpeech + EOS + Response\n",
    "            input_text = f\"{hate_speech} {judge_tokenizer.eos_token} {response}\"\n",
    "            input_ids = judge_tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = judge_model(input_ids, labels=input_ids)\n",
    "                score = -output.loss.item()  # Higher is better (negative loss)\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_intent = intent\n",
    "\n",
    "        selected_intents[hate_speech] = best_intent\n",
    "\n",
    "    return selected_intents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): Traceback (most recent call last):\n",
      "  File \"/home/dhruv/miniforge3/envs/nlp_project/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/dhruv/miniforge3/envs/nlp_project/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 57, in main\n",
      "    service.run()\n",
      "  File \"/home/dhruv/miniforge3/envs/nlp_project/lib/python3.10/site-packages/huggingface_hub/commands/user.py\", line 153, in run\n",
      "    login(\n",
      "  File \"/home/dhruv/miniforge3/envs/nlp_project/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/dhruv/miniforge3/envs/nlp_project/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 31, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/dhruv/miniforge3/envs/nlp_project/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 130, in login\n",
      "    interpreter_login(new_session=new_session)\n",
      "  File \"/home/dhruv/miniforge3/envs/nlp_project/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/dhruv/miniforge3/envs/nlp_project/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 31, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/dhruv/miniforge3/envs/nlp_project/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 287, in interpreter_login\n",
      "    token = getpass(\"Enter your token (input will not be visible): \")\n",
      "  File \"/home/dhruv/miniforge3/envs/nlp_project/lib/python3.10/getpass.py\", line 77, in unix_getpass\n",
      "    passwd = _raw_input(prompt, stream, input=input)\n",
      "  File \"/home/dhruv/miniforge3/envs/nlp_project/lib/python3.10/getpass.py\", line 146, in _raw_input\n",
      "    line = input.readline()\n",
      "  File \"/home/dhruv/miniforge3/envs/nlp_project/lib/python3.10/codecs.py\", line 319, in decode\n",
      "    def decode(self, input, final=False):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_judge_model(model, test_dataloader, device):\n",
    "    categories = {\n",
    "        'informative': 0,\n",
    "        'questioning': 1,\n",
    "        'denouncing': 2,\n",
    "        'positive': 3,\n",
    "        'humor': 4\n",
    "    }\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    judge_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "    judge_model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\").to(device)\n",
    "    judge_model.eval()\n",
    "    print(\"Judge model ready!\")\n",
    "\n",
    "    total_samples = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    test_loop = tqdm(test_dataloader, desc=\"Test Evaluation\", leave=True)\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            counter_speech = batch['counter_speech'].to(device)\n",
    "            raw_inputs = batch['raw_inputs']\n",
    "            all_intents = batch['all_intents']  # list of gold intents per sample (as int indices)\n",
    "\n",
    "            outputs = model.judge_responses(input_ids, attention_mask, counter_speech)\n",
    "\n",
    "            for i in range(len(input_ids)):\n",
    "                intent_scores = {}\n",
    "\n",
    "                for intent in [\"informative\", \"questioning\", \"denouncing\", \"positive\", \"humor\"]:\n",
    "                    response = model.tokenizer.decode(\n",
    "                        torch.argmax(outputs[intent][i].logits, dim=-1)[0],\n",
    "                        skip_special_tokens=True\n",
    "                    )\n",
    "\n",
    "                    # Prompting for a rating\n",
    "                    prompt = (\n",
    "                        f\"Hate speech: {raw_inputs[i]}\\n\"\n",
    "                        f\"Proposed counterspeech (intent: {intent}): {response}\\n\\n\"\n",
    "                        f\"On a scale of 1 to 10, how appropriate and effective is this counterspeech in response to the hate speech? Just respond with a number.\"\n",
    "                    )\n",
    "\n",
    "                    judge_input = judge_tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "                    output_ids = judge_model.generate(judge_input, max_new_tokens=10, pad_token_id=judge_tokenizer.eos_token_id)\n",
    "                    score_text = judge_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                    try:\n",
    "                        # Extract the first number in response (robust to \"I would give it a 7\" etc.)\n",
    "                        score = next((float(s) for s in score_text.split() if s.replace('.', '', 1).isdigit()), 0)\n",
    "                        score = max(0, min(score, 10))  # Clamp between 0 and 10\n",
    "                    except:\n",
    "                        score = 0  # Fallback score if judge LM fails\n",
    "\n",
    "                    intent_scores[intent] = score\n",
    "\n",
    "                # Pick best scoring intent\n",
    "                best_intent = max(intent_scores, key=intent_scores.get)\n",
    "                best_intent_idx = categories[best_intent]\n",
    "\n",
    "                if best_intent_idx in all_intents[i]:\n",
    "                    correct_predictions += 1\n",
    "                total_samples += 1\n",
    "\n",
    "            test_loop.set_postfix({'accuracy': correct_predictions / total_samples if total_samples else 0})\n",
    "\n",
    "    final_accuracy = correct_predictions / total_samples if total_samples else 0\n",
    "    print(f\"\\nFinal Category Accuracy using Judge LM (Rating 1–10): {final_accuracy:.4f}\")\n",
    "    return final_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge model ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Evaluation:   0%|                                                                                                                                                                       | 0/93 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Test Evaluation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 93/93 [06:42<00:00,  4.33s/it, accuracy=0.736]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Category Accuracy using Judge LM (Rating 1–10): 0.7358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7357791989229215"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True,collate_fn = custom_collate_fn)\n",
    "evaluate_with_judge_model(model,test_dataloader,device = torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00606f57d8884a8da09c02a2b181667c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_baa6a137ee63491cb9a5f54e44095154",
      "max": 7021,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_13cfef767914493fb7c16b42845c4c45",
      "value": 7021
     }
    },
    "1274c2be26b64ab3b4ac358f7606a310": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "13cfef767914493fb7c16b42845c4c45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "36aff3cc8ee845b08abd7f18589726fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f5df75b26d1d464fb722dff990b5476a",
       "IPY_MODEL_00606f57d8884a8da09c02a2b181667c",
       "IPY_MODEL_5ae640fb971841d9b86737e6fd0b4c78"
      ],
      "layout": "IPY_MODEL_452dc6a25a544daab3217fae74b699e9"
     }
    },
    "452dc6a25a544daab3217fae74b699e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d09db8336c340e4be9bc537537f4b0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5ae640fb971841d9b86737e6fd0b4c78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95abeb3513704f59b66711a516c6130a",
      "placeholder": "​",
      "style": "IPY_MODEL_4d09db8336c340e4be9bc537537f4b0d",
      "value": " 7.02k/7.02k [00:00&lt;00:00, 436kB/s]"
     }
    },
    "95abeb3513704f59b66711a516c6130a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "baa6a137ee63491cb9a5f54e44095154": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5df75b26d1d464fb722dff990b5476a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fccb872427ac46018921c52b405210ea",
      "placeholder": "​",
      "style": "IPY_MODEL_1274c2be26b64ab3b4ac358f7606a310",
      "value": "Downloading builder script: 100%"
     }
    },
    "fccb872427ac46018921c52b405210ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
